[
  {
    "path": "posts/Total Health NVM Research/",
    "title": "Total Health NVM Research",
    "description": "Example NVM Research",
    "author": [
      {
        "name": "Edward Hillenaar",
        "url": "https://edwardhill-th-distill.netlify.app"
      }
    ],
    "date": "2022-06-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nResults\r\nData\r\nPlot1\r\nPlot2\r\nPlot3\r\nPlot4\r\nAnimation1\r\nRegression\r\n\r\n\r\n\r\n\r\nIntroduction\r\nInclude reference in the text (Davis, Strasburger, and Brown 2007).\r\nThis is another citation (Grosz 2005). (Senez et al. 2004) More: (Reid et al.\r\n2010), (Schie 2008)\r\nEn nog meer: [Kuipers et al. (1999))\r\nAlfred North Whitehead (Guide 2003)\r\npunt 1\r\npunt 2\r\npunt 3\r\npunt 4\r\nThis is an R Markdown document. Markdown is a simple formatting\r\nsyntax for authoring HTML, PDF, and MS Word documents. For more details\r\non using R Markdown see http://rmarkdown.rstudio.com.\r\nWhen you click the Knit button a document will be\r\ngenerated that includes both content as well as the output of any\r\nembedded R code chunks within the document. You can embed an R code\r\nchunk like this:\r\nResults\r\nData\r\n\r\n\r\n\r\nPlot1\r\nNVM Research graph1Plot2\r\nNVM Research graph3Plot3\r\nNVM Research graph2Plot4\r\nPLot4Animation1\r\nAnimation jitterplot Consciousness and\r\nSomatizationYou can also embed plots, for example:\r\n\r\n\r\n\r\n\r\n\r\n\r\nRegression\r\nNote that the echo = FALSE parameter was added to the\r\ncode chunk to prevent printing of the R code that generated the\r\nplot.\r\n\r\n\r\n\r\nDavis, L W, A M Strasburger, and L F Brown. 2007.\r\n“Mindfulness.” An Intervention for Anxiety\r\nin Schizophrenia. J Psychosoc Nurs 45: 23–29.\r\n\r\n\r\nGrosz, Elizabeth. 2005. “Bergson, Deleuze and\r\nthe becoming of unbecoming.” Parallax 11 (2):\r\n4–13.\r\n\r\n\r\nGuide, Interpretation. 2003. “Corrected on\r\nDecember 19, 2011.” Diabetes Care 26 (8):\r\n2288–93.\r\n\r\n\r\nKuipers, H, H A Keizer, Tj Wiersma, R J Heine, and Others. 1999.\r\n“De standaard’Diabetes mellitus type\r\n2’(eerste herziening) van het Nederlands Huisartsen\r\nGenootschap.” Ned Tijdschr Geneeskd 143: 2342–43.\r\n\r\n\r\nReid, R D, H E Tulloch, R J Sigal, G P Kenny, M Fortier, L McDonnell, G\r\nA Wells, N G Boule, P Phillips, and D Coyle. 2010. “Effects of aerobic exercise, resistance exercise or both,\r\non patient-reported health status and well-being in type 2 diabetes\r\nmellitus: a randomised trial.” Diabetologia 53\r\n(4): 632–40.\r\n\r\n\r\nSchie, Carine H M van. 2008. “Neuropathy:\r\nmobility and quality of life.” Diabetes/Metabolism\r\nResearch and Reviews 24 (S1): S45—–S51.\r\n\r\n\r\nSenez, Bruno, Pascal Felicioli, Alain Moreau, and Marie-France Le\r\nGoaziou. 2004. “Évaluation de la\r\nqualité de vie des patients diabétiques de type 2 en médecine générale.” La Presse\r\nmédicale 33 (3): 161–66.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/Total Health NVM Research/figures/boxplot.png",
    "last_modified": "2022-06-10T09:49:00+02:00",
    "input_file": "Total-Health-NVM-Research.knit.md",
    "preview_width": 3000,
    "preview_height": 2700
  },
  {
    "path": "posts/Alfred North Whitehead/",
    "title": "Alfred North Whitehead",
    "description": "Alfred North Whitehead post with references to biblio.bib.",
    "author": [
      {
        "name": "Edward Hillenaar",
        "url": "https://edwardhill-th-distill.netlify.app"
      }
    ],
    "date": "2022-06-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nResults\r\nData\r\nPlots\r\nRegression\r\n\r\n\r\nIntroduction\r\nInclude reference in the text (Sjöstedt-Hughes 2016). This is another\r\ncitation (Segall 2019).\r\n(Kraus 1998) More: (Smith 2010), (Sjöstedt-Hughes, n.d.)\r\nEn nog meer: [Desmet\r\n(2010))\r\nAlfred North Whitehead (Schilpp 1941)\r\nAlfred North WhiteheadAlfred North Whiteheadpunt 1\r\npunt 2\r\npunt 3\r\npunt 4\r\nThis is an R Markdown document. Markdown is a simple formatting\r\nsyntax for authoring HTML, PDF, and MS Word documents. For more details\r\non using R Markdown see http://rmarkdown.rstudio.com.\r\nWhen you click the Knit button a document will be\r\ngenerated that includes both content as well as the output of any\r\nembedded R code chunks within the document. You can embed an R code\r\nchunk like this:\r\nResults\r\nData\r\n\r\n     speed           dist       \r\n Min.   : 4.0   Min.   :  2.00  \r\n 1st Qu.:12.0   1st Qu.: 26.00  \r\n Median :15.0   Median : 36.00  \r\n Mean   :15.4   Mean   : 42.98  \r\n 3rd Qu.:19.0   3rd Qu.: 56.00  \r\n Max.   :25.0   Max.   :120.00  \r\n\r\nPlots\r\nYou can also embed plots, for example:\r\n\r\n\r\n\r\nRegression\r\nNote that the echo = FALSE parameter was added to the\r\ncode chunk to prevent printing of the R code that generated the\r\nplot.\r\n\r\n\r\n\r\nDesmet, R. 2010. “Whitehead ’ s Principle of\r\nRelativity,” 1–5.\r\n\r\n\r\nKraus, Elizabeth M. 1998. The Metaphysics of\r\nExperience: A Companion to Whitehead’s Process and Reality American\r\nPhilosophy Series, 1073-2764. New York: Fordham University\r\nPress.\r\n\r\n\r\nSchilpp, Paul Arthur. 1941. The Philosophy of\r\nAlfred North Whitehead. Menasha: George Banta Publishing\r\nCompany. https://doi.org/10.5840/thought194217387.\r\n\r\n\r\nSegall, Matthew T. 2019. “Time and Experience\r\nin Physics and Philosophy: Whiteheadian Reflections on Bergson,\r\nEinstein, and Rovelli.” Aquila.\r\n\r\n\r\nSjöstedt-Hughes, Peter. 2016. “The Philosophy\r\nof Organism: Whitehead’s Organic Awareness of Reality,”\r\nno. September.\r\n\r\n\r\n———. n.d. “The Great God Pan is Not Dead:\r\nA.N. Whitehead and the Psychedelic Mode of Perception.”\r\nPsychedelic Press.\r\n\r\n\r\nSmith, Olav Bryant. 2010. “The Social Self of\r\nWhitehead’s Organic Philosophy.” European Journal of\r\nPragmatism and American Philosophy II (1): 0–15. https://doi.org/10.4000/ejpap.935.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/Alfred North Whitehead/figures/Alfred1.jpg",
    "last_modified": "2022-06-09T07:10:12+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-14-best-weather-2/",
    "title": "Cities with Best (and Worst) Weather, 2021 edition",
    "description": "An analysis of NCEI GSOD weather data with a subsequent ranking of World and U.S. cities in terms pleasant weather. Count of pleasant days and calculation of excess degree-days are used as two different approaches, returning similar (yet not identical) results.",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2021-03-14",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSummary\r\nResultsBest world cities\r\nWorst world cities\r\nLargest world cities\r\nWorld cities data table\r\nUnited States\r\nUnited States cities data tables\r\n\r\nDiscussion\r\nMethodology\r\n\r\nSummary\r\nIn this post, I examine NCEI’s GSOD (Global Summary of the Day) data (DOC/NOAA/NESDIS/NCDC n.d.) and attempt to determine which cities in the world, as well as in the United States, have the most pleasant weather for human habitation.\r\nFollowing my earlier blog posts “Where are the places with the best (and the worst) weather in the United States?” and “Measuring Excess Degree-Days in the Context of Location Comfort and Liveability”, this post will rank U.S. and world cities using two separate approaches: my original approach to classify each day as pleasant or not (hot, cold, having precipitation), as well as my calculation of excess degree-days.\r\nThe classification of days into distinct categories of pleasant / hot / cold / elements is easier from a communication standpoint, i.e. it is more intuitive. The main problem with it, however, is in its key feature: it creates distinct categories out of continuous variables. Not only that, it also employs several such breaks, each of which is arbitrary to some extend, and the final result of checking every day against this set of parameters being even more arbitrary.\r\nFor this ranking, I have expanded my original criteria to account for a greater range of people’s perceptions of weather. A pleasant day will thus be determined as one where:\r\nlowest temperature was above 0°C (32°F) but not higher than 20°C (68°F)\r\nhighest temperature was above 10°C (50°F) but not higher than 35°C (95°F)\r\nno significant amount of precipitation\r\nThe calculation of excess degree-days, on the other hand, is the reverse side of the previous algorithm in term of benefits and drawbacks. While it is much less intuitive and more complex in calculation (using trigonometry and integral calculus), it is a continuous scale that doesn’t create arbitrary breaks. Additionally, it only requires one baseline input, and while it can be somewhat subjective as to what a baseline temperature should be, the change in the baseline does not sway the calculation too much, keeping the ranking mostly in order. Aside from all this, however, the Excess Degree-Days metric (or EDD for short) does not incorporate the rain/snow variable, and only focuses on the daily temperatures.\r\nhttps://taraskaduk.com/posts/2019-11-11-excess-degree-days/For this ranking, the baseline and the calculations are in line with the above cited post: 18°C (64.5°F) is the starting point.\r\nResults\r\nEach chart is a ranking of 50 cities, best or worst, with #1 spot being best or worst, respective of the ranking (best of all best 50 or worst of all worst 50). Both “Pleasant days” and “Excess Degree-Days” charts are provided. Furthermore, “Excess Degree-Days” is rendered in two versions red-yellow-green-blue gradient, as well as colorblind-friendly red-white-blue gradient.\r\nEach circle depicts 10 years, from 2011 through 2020, with 2011 as innermost layer and 2020 as outermost, with days moving clockwise from January 1 at 12:00 position, April 1 at 3:00 position, and so on.\r\n\r\nBest world cities\r\nThese are the rankings of cities with the best weather: most pleasant days / lowest total excess-degree days.\r\n\r\n\r\n\r\nWorst world cities\r\nThese are the rankings of cities with the worst weather: fewest pleasant days / highest total excess-degree days.\r\nWhile the “best” cities overlap between the two rankings, this is clearly not the case for the other side: in the “pleasant days” ranking, we see all the cities with hot climate year-round, while the EDD ranking shows us mostly cold cities. This is due to the fact that while each day in the hottest climate can be too warm to be pleasant or comfortable, the temperature does go up as much as it goes down. I.e. a very cold day in Siberia is many more degrees away from human comfort than a very hot day in Southeast Asia.\r\nThis is not a flaw in the post, but rather a feature, and this discrepancy is a live illustration of a different mechanism of the two metrics.\r\n\r\n\r\n\r\nLargest world cities\r\nIn the final “World” segment, rather than finding “best” or “worst” cities by each metric, I isolated 50 most populous cities first, and then ranked them accordingly, best to worst.\r\n\r\n\r\n\r\nWorld cities data table\r\n\r\n\r\n\r\nPleasant days in world cities over 1,000,000 people\r\n\r\n\r\n\r\n\r\nExcess Degree-Days in world cities over 1,000,000 people\r\n\r\n\r\n\r\n\r\nPleasant days in world cities over 500,000 people\r\n\r\n\r\n\r\n\r\nExcess Degree-Days in world cities over 500,000 people\r\n\r\n\r\n\r\n\r\nUnited States\r\nFor the United States, there are 46 metro areas that are over 1,000,000 people (according to the data from simplemaps.com), and therefore I simply decided to get 50 largest total cities: this way, “cities over 1M people with best weather” and “most populous cities ranked” are the same chart, while “worst weather” ranking is the same chart read backwards: #50 to #1.\r\n\r\n\r\n\r\nUnited States cities data tables\r\nPleasant days in United States cities over 1,000,000 people\r\n\r\n\r\n\r\n\r\nExcess Degree-Days in United States cities over 1,000,000 people\r\n\r\n\r\n\r\n\r\nPleasant days in United States cities over 500,000 people\r\n\r\n\r\n\r\n\r\nExcess Degree-Days in United States cities over 500,000 people\r\n\r\n\r\n\r\n\r\nDiscussion\r\nA person familiar with locations of many top cities in these ratings will recognize that many of these places are located in in rather specific zones: proximity a sea/ocean and elevation being two factors that deliver mild and pleasant climate.\r\nIndeed, determining Köppen climate zones for all locations and then finding an average amount of pleasant days and average amount of excess degree-days shows that cities with most pleasant weather (by any of the two metrics) are located in Cwb (Subtropical highland) and Csb (Warm-summer Mediterranean) climate zones: both very mild temperate climates.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMethodology\r\nAll code required for this analysis can be found in the taraskaduk/weather repo. All analysis, with the exception of the plot generation, is written is a {drake} plan (Landau 2018). A more detailed post explaining the process is in the works, but in a meantime, the basic outline of the process is the following:\r\nidentify locations of interest\r\nfind nearest weather stations for each location using {GSODR} package (Sparks, Hengl, and Nelson 2017)\r\ndownload and transform data for selected stations using a combination of base R and {GSODR}\r\nperform further clean-up and aggregation by location using numerous packages, including but not limited to {tidyverse} (Wickham et al. 2019), {sf} (Pebesma 2018) and others\r\napply “pleasant days” classification (Kaduk 2019b) and Excess Degree-Days calculation (Kaduk 2019a), with the parameters listed in the Summary section of this post.\r\n\r\n\r\n\r\nDOC/NOAA/NESDIS/NCDC. n.d. “Global Surface Summary of the Day - GSOD.” DOC/NOAA/NESDIS/NCDC. Accessed February 18, 2019. https://data.noaa.gov/dataset/dataset/global-surface-summary-of-the-day-gsod.\r\n\r\n\r\nKaduk, Taras. 2019a. “Taras Kaduk: Measuring Excess Degree-Days in the Context of Location Comfort and Liveability.” https://taraskaduk.com/posts/2019-11-11-excess-degree-days/.\r\n\r\n\r\n———. 2019b. “Taras Kaduk: Where Are the Places with the Best (and the Worst) Weather in the United States?” https://taraskaduk.com/posts/2019-02-18-weather/.\r\n\r\n\r\nLandau, William Michael. 2018. “The Drake R Package: A Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 3 (21). https://doi.org/10.21105/joss.00550.\r\n\r\n\r\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\r\n\r\n\r\nSparks, Adam H, Tomislav Hengl, and Andrew Nelson. 2017. “GSODR: Global Summary Daily Weather Data in R.” The Journal of Open Source Software 2 (10). https://doi.org/10.21105/joss.00177.\r\n\r\n\r\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-14-best-weather-2/figures/50_most_world_0_10_edd2.png",
    "last_modified": "2022-06-06T14:01:26+02:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 1380
  },
  {
    "path": "posts/2021-01-18-print-street-maps/",
    "title": "Print Personalized Street Maps Using R",
    "description": "A code walk-through of how to generate streets maps in R using {osmdata} and {sf} packages.\nInitially published on 2019-12-19 and updated on 2020-01-18.",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2021-01-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n2021-01-18 Update\r\nIntroduction\r\nThe ProcessObtain data\r\nFinding bordersScenario 1: Go by the city’s borders\r\nScenario 3.1: Rectangular map, known margins\r\nScenario 3.2: Rectangular map, known center, known radius, unknown margins\r\nScenario 3.3: Rectangular map, with points on the map (memorable places, landmarks, etc)\r\n\r\nCutting the map\r\nPlotting\r\n\r\n\r\n2021-01-18 Update\r\nI had this post published originally on 2019-12-19 on my old version of the website, which ran on HUGO. As I was migrating the content from HUGO to Distill, I purged a lot of stuff that I deemed unnecessary, this post being among the purged ones. By that time, I found several really good online guides on how to print street maps more efficiently (McCrain 2020) (Burkhart, n.d.). However, a few people12 asked about my original post since then, and I felt that I owe it to the intrawebs to put it back on.\r\n\r\nFramed and hung! Data as an art form! #rstats #dataviz #maps #gis https://t.co/TFNOAanbzb pic.twitter.com/1lR48CwEhD— Taras Kaduk (@taraskaduk) December 19, 2019\r\n\r\n\r\nIntroduction\r\nIn this tutorial, I will walk you through the process of creating street maps using nothing but R code. After going through the steps in the post, you should be able to get the map of the city of your choice. You can then order prints of your output.3\r\nThe Process\r\nsf (Pebesma 2018) and osmdata (Padgham et al. 2017) are the working horses of this process, with tidyverse (Wickham et al. 2019) providing the tools to manipulate the data. The rest of the packages handle one-off jobs in the process.\r\n\r\n\r\nlibrary(sf)\r\nlibrary(osmdata)\r\nlibrary(raster)\r\nlibrary(tidyverse)\r\nlibrary(lwgeom)\r\n\r\n\r\n\r\nObtain data\r\nIn the initial version of this post, I relied on Geofabrik Download Server to get OSM data for the city of my interest. Since then, I discovered osmdata package as a much better alternative to my original process.\r\nI will be creating a map of Kyiv, Ukraine, but the process should be identical for any other city (except for mapping the bodies of water, which can get tricky)\r\n\r\n\r\nplace <- \"Kyiv Ukraine\"\r\n\r\nhighway_sizes <- tibble::tribble(\r\n          ~highway, ~highway_group, ~size,\r\n        \"motorway\",        \"large\",   0.5,\r\n   \"motorway_link\",        \"large\",   0.3,\r\n         \"primary\",        \"large\",   0.5,\r\n    \"primary_link\",        \"large\",   0.3,\r\n       \"secondary\",       \"medium\",   0.3,\r\n  \"secondary_link\",       \"medium\",   0.3,\r\n        \"tertiary\",       \"medium\",   0.3,\r\n   \"tertiary_link\",       \"medium\",   0.3,\r\n     \"residential\",        \"small\",   0.2,\r\n   \"living_street\",        \"small\",   0.2,\r\n    \"unclassified\",        \"small\",   0.2,\r\n         \"service\",        \"small\",   0.2,\r\n         \"footway\",        \"small\",   0.2\r\n  )\r\n\r\n\r\nstreets_osm <- opq(place) %>%\r\n  add_osm_feature(key = \"highway\", \r\n                  value = highway_sizes$highway) %>%\r\n  osmdata_sf()\r\n\r\nstreets <- streets_osm$osm_lines %>% \r\n  select(osm_id, name, name.en, highway, maxspeed, oneway, surface) %>% \r\n  mutate(length = as.numeric(st_length(.))) %>% \r\n  left_join(highway_sizes, by=\"highway\") %>% \r\n  filter(highway_group != \"small\" | length >= quantile(length, probs = 0.25))\r\n\r\nrailways_osm <- opq(place) %>%\r\n  add_osm_feature(key = \"railway\", value=\"rail\") %>%\r\n  osmdata_sf()\r\n\r\nrailways <- railways_osm$osm_lines %>% \r\n  dplyr::select()\r\n\r\n\r\n\r\nFor water, a lot depend on how you want to map it. If you only want to show the river as a line, you shouldn’t have a problem with key-value pair of “waterway”-“river”.\r\nI wanted to map the Dnipro river (the river that runs through Kyiv, as well as through all of Ukraine) as it is on the map. My code below accomplishes that, but it may not work for your body of water. Please do research on key-value pairs for your case then.\r\n\r\n\r\nwater_osm <- opq(place) %>%\r\n  add_osm_feature(key = \"natural\", value = \"water\") %>%\r\n  osmdata_sf() %>% \r\n  unname_osmdata_sf()\r\n\r\nriver_osm <- opq(place) %>%\r\n  add_osm_feature(key = \"waterway\", value = c(\"river\", \"riverbank\")) %>%\r\n  osmdata_sf() %>% \r\n  unname_osmdata_sf()\r\n\r\nwater <- c(water_osm, river_osm) %>% \r\n  .$osm_multipolygons %>% \r\n  select(osm_id, name) %>% \r\n  mutate(area = st_area(.)) %>% \r\n  # this filter gets rid of tiny isolated lakes et cetera\r\n  filter(area >= quantile(area, probs = 0.75))\r\n\r\n\r\n\r\nFinding borders\r\nThis is by far the most complicated part of the code. How to determine which part of these shapefiles to print? You have roads and water data for the city, and that data goes beyond the actual city limits. At the same time, do you want the entire city? Do you want zoom in or zoom out? There are many ways to go about this problem, and I’ll try to list a few solutions.\r\nScenario 1: Go by the city’s borders\r\nPerhaps, the easiest approach is to go by the city’s (or metro area’s) borders. In the existing workflow, these are provided, I just needed to find what I wanted:\r\n\r\n\r\n#First, get all admin boundaries associated with your request\r\nall_boundaries <- opq(place) %>%\r\n  add_osm_feature(key = \"boundary\", \r\n                  value = c(\"administrative\")) %>%\r\n  osmdata_sf() %>% \r\n  #This next step is optional, depending on what the call returns\r\n  unname_osmdata_sf() %>% \r\n  .$osm_multipolygons\r\n\r\nggplot(data = all_boundaries) + \r\n  geom_sf()\r\n\r\n\r\n\r\n#The previous call returns more than one boundary.\r\n#Inspect the data and choose the object you need.\r\nboundary <- all_boundaries %>% \r\n  filter(osm_id == 421866) %>% \r\n  #If you're after geometries only, \r\n  #you won't need all other 200 some columns\r\n  dplyr::select()\r\n\r\n\r\n\r\n\r\n\r\n\r\nScenario 2: A circular map\r\nThe algorithm for this approach is to find a center point on the map, then draw a circle around that center point. The inspiration and the code for this approach came from “The Beautiful Hidden Logic of Cities” (Davis 2019).\r\nYou will need to know your center’s coordinates, the desired radius in meters, as well as your local crs projection (to re-project the coordinates in order to draw a circle with a radius given in meters).\r\n\r\n\r\ncrs2 <- 6384 # https://epsg.io/6384\r\n\r\ncenter <- c(long = 30.522,\r\n            lat = 50.451)\r\ncenter_proj <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%\r\n  st_transform(crs = crs2)\r\n\r\ndist <-  10000\r\ncircle <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%\r\n  st_transform(crs = crs2) %>% \r\n  st_buffer(dist = dist) %>% \r\n  st_transform(crs = 4326)\r\n\r\n\r\n\r\n\r\n\r\n\r\nScenario 3.1: Rectangular map, known margins\r\nTo cut out a clear square or rectangle, there is a bit more work to do. The easiest way is if we know the margins — in that case, we only need to pass the marginal latitudes and longitudes to a bbox variable:\r\n\r\n\r\nbbox <- c(xmin= your_lowest_lat, \r\n          ymin=your_leftmost_long, \r\n          xmax= your_highest_lat, \r\n          ymax= your_rightmost_long)\r\n\r\n\r\n\r\nScenario 3.2: Rectangular map, known center, known radius, unknown margins\r\nIf we know a desired center, and know (or guess) a radius, constructing a crop box becomes a matter of drawing a circle around the center, and then finding the tangents on each of four sides.\r\n\r\n\r\ndist <-  10000\r\ncircle1 <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%  \r\n  st_transform(crs = crs2) %>% \r\n  st_buffer(dist = dist)\r\nbbox1 <- st_bbox(circle1)\r\n\r\n\r\n\r\nAn extra complication: if we don’t want a square, but a rectangular map. I wanted a 4:5 ratio. What to do? Draw another circle, and then use 2 data points from each to form a box.\r\n\r\n\r\ncircle2 <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%  \r\n  st_transform(crs = crs2) %>% \r\n  st_buffer(dist = dist/0.8) %>% \r\n  st_transform(crs = 4326)\r\n\r\nbbox2 <- st_bbox(circle2)\r\nbbox <- c(bbox1$xmin, bbox2$ymin, bbox1$xmax, bbox2$ymax)\r\n\r\n\r\n\r\nScenario 3.3: Rectangular map, with points on the map (memorable places, landmarks, etc)\r\nThis is the longest of all possible scenarios.\r\nIf you are going to plot some personal data points on top, this is the time upload your data. Here is a sample I used:\r\n\r\n\r\npoints <- tibble::tribble(\r\n             ~Place,      ~Lat,     ~Long, ~Type, ~Person,\r\n            \"Abcde\", 50.433374, 30.426781,   \"A\",     \"A\",\r\n            \"Abcde\", 50.500973,  30.36365,   \"A\",     \"A\",\r\n            \"Abcde\", 50.456738, 30.622579,   \"B\",     \"A\",\r\n            \"Abcde\", 50.427142, 30.468178,   \"C\",     \"A\",\r\n            \"Abcde\", 50.427348, 30.423886,   \"C\",     \"A\",\r\n            \"Abcde\", 50.425519, 30.436046,   \"C\",     \"A\",\r\n            \"Abcde\", 50.504877, 30.469968,   \"D\",     \"A\",\r\n            \"Abcde\", 50.455176, 30.524202,   \"D\",     \"A\",\r\n            \"Abcde\",  50.42226, 30.425524,   \"E\",     \"A\",\r\n            \"Abcde\", 50.428142, 30.430448,   \"E\",     \"A\",\r\n            \"Abcde\", 50.439139, 30.553577,   \"A\",     \"B\",\r\n            \"Abcde\",  50.40741, 30.401694,   \"A\",     \"B\",\r\n            \"Abcde\", 50.520441,  30.49738,   \"A\",     \"B\",\r\n            \"Abcde\", 50.397799, 30.513417,   \"A\",     \"B\",\r\n            \"Abcde\", 50.447156, 30.431727,   \"B\",     \"B\",\r\n            \"Abcde\", 50.427097, 30.519814,   \"B\",     \"B\",\r\n            \"Abcde\", 50.516017,  30.60801,   \"E\",     \"B\",\r\n            \"Abcde\", 50.509067,  30.46301,   \"A\",     \"C\",\r\n            \"Abcde\", 50.454697, 30.505255,   \"C\",     \"C\",\r\n            \"Abcde\", 50.450976, 30.522568,   \"D\",     \"C\",\r\n            \"Abcde\", 50.420312, 30.528391,   \"D\",     \"C\",\r\n            \"Abcde\", 50.459616, 30.521638,   \"D\",     \"C\",\r\n            \"Abcde\", 50.437684, 30.527156,   \"D\",     \"C\",\r\n            \"Abcde\", 50.457284, 30.438777,   \"E\",     \"C\",\r\n            \"Abcde\", 50.500091, 30.602264,   \"E\",     \"C\"\r\n            )\r\npoints_sf <- points %>% \r\n  st_as_sf(coords = c(\"Long\", \"Lat\"), crs = 4326)\r\n\r\n\r\n\r\nAnother thing that we need to do now is to make sure that all points fit inside the printed map. We’re going to find the limits on all sides. Out of curiosity, we also can find the center of such map.\r\n\r\n\r\ntop <- max(points$Lat)\r\nbottom <- min(points$Lat)\r\nleft <- min(points$Long)\r\nright <- max(points$Long)\r\n\r\ncenter <- c(long = (right - left)/2 + left,\r\n            lat = (top - bottom)/2 + bottom)\r\n\r\n\r\n\r\nMy approach is slightly different and more complicated though. I have a predetermined center in mind (and I assume you will as well), but I also want to make sure all of my points fit in the map, while center remains center. I need to find the longest of four distances from the center to the top, bottom, left, and right, and use that as my new length. I also will be adding an extra kilometer to the distance to have the furthest point not completely on the edge of the map\r\n\r\n\r\ncenter <- c(long = 30.5224974,\r\n            lat = 50.4508911)\r\n\r\ntop1 <- pointDistance(center, c(center[\"long\"], top), lonlat = TRUE)\r\nbottom1 <- pointDistance(center, c(center[\"long\"], bottom), lonlat = TRUE)\r\nright1 <- pointDistance(center, c(right, center[\"lat\"]), lonlat = TRUE)\r\nleft1 <- pointDistance(center, c(left, center[\"lat\"]), lonlat = TRUE)\r\n\r\ndist <- max(top1,\r\n            bottom1, \r\n            right1, \r\n            left1) + 1000 \r\n\r\n\r\n\r\nNow that we have a center and a radius, we can draw the box the same way we did in Scenario 3.2.\r\n\r\n\r\ncircle1 <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%  \r\n  st_transform(crs = crs2) %>% \r\n  st_buffer(dist = dist) %>% \r\n  st_transform(crs = 4326)\r\n\r\nbbox1 <- st_bbox(circle1)\r\n\r\ncircle2 <- tibble(lat = center[\"lat\"], long = center[\"long\"]) %>% \r\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%  \r\n  st_transform(crs = crs2) %>% \r\n  st_buffer(dist = dist/0.8) %>% \r\n  st_transform(crs = 4326)\r\n\r\nbbox2 <- st_bbox(circle2)\r\nbbox <- c(bbox1$xmin, bbox2$ymin, bbox1$xmax, bbox2$ymax)\r\n\r\n\r\n\r\nCutting the map\r\nWith borders determined (one of several ways above), we are ready to crop the map to its desired size. If we’ve been cutting out a rectangular shape, we can use a st_crop() function to crop the shapefiles. Otherwise, we’ll need st_intersection()\r\nIf you want to you the city/metro/district/etc borders, the code and the initial output will look something like this:\r\n\r\n\r\nstreets_cropped <- streets %>% st_intersection(boundary)\r\nwater_cropped <- water %>% st_intersection(boundary)\r\nrailways_cropped <- railways %>% st_intersection(boundary)\r\n\r\n\r\n\r\n\r\n\r\n\r\nA circle crop will have a very similar code, and will most likely run much faster:\r\n\r\n\r\nstreets_cropped <- streets %>% st_intersection(circle)\r\nwater_cropped <- water %>% st_intersection(circle)\r\nrailways_cropped <- railways %>% st_intersection(circle)\r\n\r\n\r\n\r\n\r\n\r\n\r\nA rectangular crop will have a slightly different code, as mentioned before, and will run even faster:\r\n\r\n\r\nwater_cropped <- st_crop(water, bbox)\r\nstreets_cropped <- st_crop(streets, bbox)\r\nrailways_cropped <- st_crop(railways, bbox)\r\n\r\n\r\n\r\n\r\n\r\n\r\nPlotting\r\nNow, to the plotting. This is where most of the tweaking is going to happen. Pick colors, opacity, width of the streets, order of layers etc etc. At some point, I wrote a double-layered for loop to iterate over several shades of gray and several versions of street widths, generating 64 different chart combinations. And that’s just for 2 parameters alone with values very close to one another! The possibilities here are endless. Have fun customizing the look!\r\nHere is my final version:\r\n\r\n\r\nblankbg <-theme(axis.line=element_blank(),\r\n                axis.text.x=element_blank(),\r\n                axis.text.y=element_blank(),\r\n                axis.ticks=element_blank(),\r\n                axis.title.x=element_blank(), \r\n                axis.title.y=element_blank(),\r\n                legend.position = \"none\",\r\n                plot.background=element_blank(),\r\n                panel.grid.minor=element_blank(),\r\n                panel.background=element_blank(),\r\n                panel.grid.major=element_blank(),\r\n                plot.margin = unit(c(t=2,r=2,b=2,l=2), \"cm\"),\r\n                plot.caption = element_text(color = \"grey20\", size = 142, \r\n                                            hjust = .5, face = \"plain\", \r\n                                            family = \"Didot\"),\r\n                panel.border = element_blank()\r\n)\r\n\r\n\r\np <- ggplot() +\r\n  blankbg +\r\n  geom_sf(data = water,\r\n          fill = \"steelblue\",\r\n          # size = .8,\r\n          lwd = 0,\r\n          alpha = .3) +\r\n  geom_sf(data = railways,\r\n          color = \"grey30\",\r\n          size = .2,\r\n          linetype=\"dotdash\",\r\n          alpha = .5) +\r\n  geom_sf(data = streets %>% \r\n            filter(highway_group == \"small\"),\r\n          size = .1,\r\n          color = \"grey40\") +\r\n  geom_sf(data = streets %>% \r\n            filter(highway_group == \"medium\"),\r\n          size = .3,\r\n          color = \"grey35\") +\r\n  geom_sf(data = streets %>% \r\n            filter(highway_group == \"large\"),\r\n          size = .5,\r\n          color = \"grey30\") +\r\n  geom_sf(\r\n    data = points_sf,\r\n    aes(#size = Size,\r\n      col = Person),\r\n    alpha = 0.8,\r\n    size = 2\r\n  ) +\r\n  labs(caption = 'Kyiv') +\r\n  coord_sf(xlim = c(bbox[1], bbox[3]),\r\n           ylim = c(bbox[2], bbox[4]),\r\n           expand = FALSE)\r\n\r\n\r\n\r\nAnd, of course, save the plot on hard drive to printing.\r\n\r\n\r\nggsave(\"kyiv_map.png\", plot=p, width = 297, height = 420, units = \"mm\", dpi = \"retina\")\r\nggsave(\"map.svg\", plot=p)\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis is pretty much it! From here, you can take the file into a program that works with vector images (e.g. Adobe Illustrator, Inkspace, Affinity Designer et cetera) to further tweak it by hand before ordering prints.\r\nEnjoy!\r\n\r\n\r\n\r\nBurkhart, Christian. n.d. “Streetmaps.” https://ggplot2tutor.com/streetmaps/streetmaps/.\r\n\r\n\r\nDavis, Erin. 2019. “The Beautiful Hidden Logic of Cities.” July 27, 2019. https://erdavis.com/2019/07/27/the-beautiful-hidden-logic-of-cities/.\r\n\r\n\r\nMcCrain, Josh. 2020. “Pretty Street Maps in R with the Osmdata Package and Ggplot.” March 17, 2020. https://joshuamccrain.com/tutorials/maps/streets_tutorial.html.\r\n\r\n\r\nPadgham, Mark, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. “Osmdata.” The Journal of Open Source Software 2 (14). https://doi.org/10.21105/joss.00305.\r\n\r\n\r\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\r\n\r\n\r\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\r\n\r\n\r\nhttps://twitter.com/tamara_cheng/status/1312697647099244544↩︎\r\nhttps://twitter.com/heyitsmehugo/status/1341874708464197632↩︎\r\nI personally do not recommend printing at home, unless you are equipped with an art printer capable of printing A3 format. As my spouse is a visual artist, among many other things, we have a large art printer and a variety of best art and archival paper, as well as tools for cutting and framing art. Nevertheless, I find A3 to be very small for street maps like this. I suggest finding a local print shop and taking your final results there.↩︎\r\n",
    "preview": "posts/2021-01-18-print-street-maps/preview.png",
    "last_modified": "2022-06-06T14:01:26+02:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 1199
  },
  {
    "path": "posts/2020-12-06-uber-movement-kyiv/",
    "title": "Analysis of Kyiv Road Traffic Using Uber Movement Data",
    "description": "This analysis uses Uber Movement speed data to analyze traffic of Kyiv, Ukraine. By looking at the speed data, we are able to find traffic flow's bottlenecks, as well as see the impact of COVID-19-related measures on the city traffic",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2020-12-06",
    "categories": [],
    "contents": "\r\nTable of Contents\r\nIntroduction\r\nData, Materials, Methods\r\nResults\r\nDiscussion\r\nIntroduction\r\nThe COVID-19 pandemic has impacted many areas of life far beyond human health. It has already had a major impact of economy, ecology, education, technology, finance etc - all too many to enumerate. It will remain a phenomenon that scientists will study for decades to come.\r\nAmong some of the impacted domains is the urban life, and in particular - how do the COVID-19-related restrictions alter city traffic and congestion. Plenty of research in this area has already been done (Romei and Burn-Murdoch 2020), (Plumer and Popovich 2020), (Geotab Data & Analytics Team 2020), (Wang et al. 2020), (Abu-Rayash and Dincer 2020). One of the most prominent features of such analyses, however, is that the data used in them is either proprietary, exists behind a paywall or a “freemium”-type arrangement, or is pre-aggregated.\r\nIn this article, I will attempt to examine the impact of COVID-19-related restrictions of city traffic using the data that is available for free: Uber Movement traffic data\r\nData, Materials, Methods\r\nAs mentioned above, for this analysis, I used data provided by Uber Movement (Uber Technologies, Inc 2020). Other data sources, such as TomTom Historical Traffic Stats, on which more elaborate traffic studies are based (TomTom 2020) (Romei and Burn-Murdoch 2020), exists. However, obtaining such data points are often associated with pay walls of “freemium” developer accounts, which is completely understandable, yet which at the same time complicates the progress of citizen data science.\r\nThe benefit of working with Uber Movement data is its free availability. The downside is in the limits of traffic data’s availability: at the time of this writing in October 2020, some Uber Movement data is available for only 60 cities worldwide and mostly up through 2020-03-31.\r\nInitial Data\r\nObtaining the data can be done via NPM Uber Movement Data Toolkit, or directly from https://movement.uber.com/ as .csv files.\r\nThe streets data contains multiple road segments each several meters long and identified by OSM IDs: way ID, start node ID, end node ID. An additional step was taken to only include road segment within administrative city boundaries.\r\nAs far as speed data, Uber Movement can provide daily data on an “hour” grain level for the above mentioned road segments. Variables provided are mean speed and speed standard deviation. To summarize, each observation includes mean speed and standard deviation for a particular short road segment on a specific hour of a specific day.\r\nData Transformation\r\nTo get the final metric of traffic delay (expressed as a %), a series of data transformation steps was taken:\r\neach segment’s length was obtained from the geometry data;\r\ngiven each segment’s length and mean speed, average travel was obtained;\r\nfor every segment, using daytime weekend data1, maximum speed was obtained for every Saturday and Sunday, and the mean of these maximum weekend daytime speeds was established as segment’s benchmark speed.\r\nwith maximum speeds available, “best time” was calculated for every observation, and the difference between actual travel time and “best time” constituted a time delay.\r\nin every subsequent grouping and aggregation, delays and “best” times would be summed up, and diving total delay time by total “best time” produced the final metric.\r\nCode\r\nDue to a large size of data necessary to work on this project, the analysis has been broken down into several parts:\r\nThe geospatial data about Kyiv city is pulled from OSM using osmdata package (Padgham et al. 2017). The code and the output are stored in taraskaduk/kyiv_osm repo on GitHub\r\nThe raw data from Uber Movement was transformed and both the code and the output are stored in taraskaduk/uber-movement repo on GitHUb\r\nThe final aggregations and visualizations are saved in the RMarkdown document that generates this web page. It should be located within the taraskaduk/taraskaduk-distill repository responsible for publishing the entire site\r\nResults\r\nTypical traffic patters\r\nFrom the most recent available data set from January 1, 2020 to March 31, 2020, 2 months between January 12 and March 12 can be used to display the recent typical traffic: January 1-14 are considered holidays and March 12 through the end of the data set were the dates under COVID-19 quarantine.\r\nThe typical traffic in Kyiv has all expected attributes of any large city: increased congestion during week days, especially during rush hours.\r\n\r\n\r\n\r\nSpatially, a typical weekday traffic flow can be visualized in the following manner:\r\n\r\n\r\n\r\nThe most problematic segments during rush hour can be visualized as follows:\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe pattern here is clear: the heaviest morning traffic at 8:00 is experienced at bridges and main highways moving people from the edges of the city into CBD, while the heaviest evening traffic at 18:00 is concentrated in the city center’s, with a literal gridlock of cars trying to get out.\r\nAggregating the data from small segments to full streets, we can identify the streets that are backed up the most overall, at the 8:00 peak and at the 18:00 peak.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCOVID-19 Quarantine Impact\r\nWhat we can also imply from these Uber Movement data sets is how the traffic was affected following the initial lockdown measures put in place to prevent the spread of COVID-19, with most countries reacting around February-March 2020.\r\nThe initial lockdown in Kyiv was initiated on March 12, 2020, which is very clear from the average delays on the roads of the city.\r\nDaily traffic delays decreased immediately after the imposed lockdown:\r\n\r\n\r\n\r\nThe effect is also visible on the hourly level, and for every day of the week, including weekends:\r\n\r\n\r\n\r\n\r\n\r\n\r\nDiscussion\r\nUnfortunately, the data provided by Uber Movement only goes up to March 31, 2020. Therefore, at this moment in time, it becomes impossible to evaluate the long-term effect of the lockdown, the easing of the restrictions, or the public fatigue on the traffic delays.\r\nNevertheless, this analysis provides a basis for such future examination, whenever more recent data becomes available. The analysis also allows to perform a similar analysis for any of the 60 cities for which the data is available. The R code used to create this analysis can be repurposed for other cities with minimal changes (see “Code” section for links to the R code behind this analysis).\r\n\r\n\r\nAbu-Rayash, Azzam, and Ibrahim Dincer. 2020. “Analysis of Mobility Trends During the COVID-19 Coronavirus Pandemic: Exploring the Impacts on Global Aviation and Travel in Selected Cities.” Energy Research & Social Science 68 (October): 101693. https://doi.org/10.1016/j.erss.2020.101693.\r\n\r\n\r\nGeotab Data & Analytics Team. 2020. “The Impact of COVID-19 on Congestion and Commercial Traffic in Cities.” Geotab. https://www.geotab.com/blog/congestion-and-commercial-traffic/.\r\n\r\n\r\nPadgham, Mark, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. “Osmdata.” The Journal of Open Source Software 2 (14). https://doi.org/10.21105/joss.00305.\r\n\r\n\r\nPlumer, Brad, and Nadja Popovich. 2020. “Traffic and Pollution Plummet as U.S. Cities Shut down for Coronavirus.” The New York Times, March. https://www.nytimes.com/interactive/2020/03/22/climate/coronavirus-usa-traffic.html.\r\n\r\n\r\nRomei, V, and J Burn-Murdoch. 2020. “Real-Time Data Show Virus Hit to Global Economic Activity.” Financial Times, Available at: Https://Www.ft.com/Content/D184fa0a-6904-11ea-800d-Da70cff6e4d3 (Accessed 30 April 2020).\r\n\r\n\r\nTomTom. 2020. “Helsinki Study on Traffic Flow Relies on TomTom Historical Traffic Data.” https://download.tomtom.com/open/banners/Helsinki-Case-Study-Traffic-Stats.pdf.\r\n\r\n\r\nUber Technologies, Inc. 2020. “Uber Movement.” https://movement.uber.com/.\r\n\r\n\r\nWang, Ding, Brian Yueshuai He, Jingqin Gao, Joseph Y. J. Chow, Kaan Ozbay, and Shri Iyer. 2020. “Impact of COVID-19 Behavioral Inertia on Reopening Strategies for New York City Transit.” arXiv:2006.13368 [Physics, Q-Fin], June. http://arxiv.org/abs/2006.13368.\r\n\r\n\r\nNighttime data was avoided due to an increased probability of excessive speeding, which would skew the data. Instead, weekend daytime speeds were analyzed to establish the benchmark↩︎\r\n",
    "preview": "posts/2020-12-06-uber-movement-kyiv/temp/preview.jpg",
    "last_modified": "2022-06-06T14:01:25+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-11-excess-degree-days/",
    "title": "Measuring Excess Degree-Days in the Context of Location Comfort and Liveability",
    "description": "Calcualting deviations (excess heat and  excess cold) from an established temperature baseline as area under the temperature curve via integral calculus and expressing such measure as Excess Degree-Days, or EDD.",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2019-11-11",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nMethodology\r\nResults\r\nNotes\r\nR code for EDD\r\n\r\nIntroduction\r\nCharacterization of daily air temperature as it relates to human comfort and conditions optimal for survival has many solutions. On the one hand, the mean daily, monthly, and annual air temperatures convey the necessary information with one number. However, as it is the case with many measures of central tendency, mean temperature loses a lot of important information, such as minimum and maximum values, and the range. On the other hand, minimum and maximum values inform the public of the temperature on a given day, but don’t communicate the extremity of said temperature readings well enough. Additionally, both daily min and max values, when aggregated to average minimums and average maximums, become more abstract.\r\nFinally, a measure that is being used more frequently in recent years and is designed to communicate the dangers of climate change is the amount of days with temperature above a certain threshold. For instance, in the articles published in the United States, “days with temperature above 90°F/95°F” is frequently used (Plumer and Popovich 2017) (Climate Impact Lab 2018) (Livingston 2020). Such indicator serves its purpose relatively well, but it loses the complexity of temperature variation and extremes and potentially evens out places with different weather and temperature readings.\r\nOn the other hand, there exist indexes and calculations that reflect the weather (and temperature in particular) not in degrees or in days when a certain condition is met, but in degree-hours or degree-days, quantifying the exposure to heat energy over time. While methodologies vary based on the domain and task at hand, the key idea is to present the difference between an established baseline and actual temperature readings as area, and then to quantify that area (Thom 1952).1\r\nWhile a lot of applications of degree-hours and degree-days seem to be focused in pest control (Zalom et al. 1983), forensics (Megyesi, Nawrocki, and Haskell 2005), and vegetation research, some use of such calculations has been applied to humans (Lin et al. 2019). I propose to use this approach as an alternative approach in evaluating the exposure to excess heat and excess cold in excess heat degree-days, excess cold degree days, and total excess degree-days, where the baseline can be set as temperature optimal for human habitation, and the actual temperature curve is presented as a sine and is calculated from minimum and maximum readings for a given day. Then, quantifying excess heat, cold, and total degree-days can be accomplished with integral calculus as finding the area under the curve.\r\nMethodology\r\nFor the purposes of this analysis, several assumptions and simplifications must be made. The daily temperature is assumed to follow a sinusoidal curve from its minimum value to its maximum value and back.2 More specifically, the daily temperature curve is modeled as a cosine function, with the minimum temperature assumed at midnight and maximum temperature at noon:3\r\n\\[f(\\theta) = -a \\cdot cos(b\\theta) + d = -\\frac{({t_{max}} - {t_{min}})}{2}\\cdot cos(\\frac{\\pi}{12}\\theta) + t_{min} + \\frac{(t_{max}-t_{min})}{2}\\]\r\nThe baseline temperature from which the excess degree-days is calculated is assumed at 18°C, which is a mid-point between a slightly colder temperature optimal for sleep, and a slightly warmer temperature optimal for daytime activity. It needs to be pointed out that such baseline is not entirely objective, as there is no agreement on what the optimal temperature for humans is, as well as there is no consideration given to most other parameters that impact the perception of outdoor air temperature: sunlight exposure, wind, precipitation etc.\r\nTo sum up, 18°C is the baseline, and any deviation up or down from which will make a human experience of outside temperature less favorable. Along the same lines, any deviation from 18°C up or down will also make humans rely on other advances of civilization: clothing, indoor insulation, heating, air conditioning, more clothing layers, more heating etc. Thus, the higher the excess degree-days reading, the less livable the place is, or the more reliant the place is on things like air conditioning or central heating.\r\nExcess Degree-Hours and Excess Degree-Days Calculation\r\nThe calculation of Excess Degree-Hours (or \\(EDH\\) for short) for the general case where the temperature curve crosses the baseline two times on a given day can be notated as:\r\n\\[EDH = \\int_{0}^{\\theta_1} (g(\\theta) - f(\\theta)) d\\theta + \\int_{\\theta_1}^{\\theta_2} (f(\\theta) - g(\\theta)) d\\theta + \\int_{\\theta_2}^{24} (g(\\theta) - f(\\theta)) d\\theta\\]\r\nwhere \\(\\theta\\) is time, \\(f(\\theta)\\) is our cosine function, \\(g(\\theta) = 18°C\\) (the baseline), and \\(\\theta_1\\) and \\(\\theta_2\\) are the times at which the temperature curve crosses the baseline.\r\n\r\n\r\n\r\nThe area above the baseline stands for excess heat degree-days, and the area below the baseline - for excess cold degree-days. The sum of absolute values of both hot and cold areas will represent the total excess degree-days.\r\nThe cases when the curve stays completely above or below the baseline are special cases that require only one of the three integrals.\r\nTo get Excess Degree-Days (\\(EDD\\)), the \\(EDH\\) needs to be divided by 24: \\[ EDD = \\frac{EDH}{24}\\]\r\nCalculation examples\r\nTo demonstrate the way this measure works, I will use 3 cities - Los Angeles, Miami, Minneapolis - at 2 specific dates: Feb 1 and Jul 1 of 2018.\r\nGiven the min and max temperature values for each city for both days, we can model the temperature curves:\r\n\r\nWe can then proceed with calculating the area under each curve:\r\n\r\nLooking more broadly at full year 2018, we can visualize the cumulative excess degree-hours:\r\n\r\nResults\r\nThis article sets a theoretical basis for further practical applications. It provides another tool in the toolbox of data analysis as it relates to climate change, weather, urban comfort and livability.\r\nMy plan is to build on this work in the future, but given the time constraints, this may happen at some time in the future. Until then, I am hoping this approach will be useful to other researchers.\r\nU.S. CBSAs ranked by excess degree-days\r\nOne quick demonstration of the EDD calculation can be done with displaying the EDD for the United States CBSAs. I will use the data from another project I’ve been working on:4\r\n\r\n\r\n\r\n\r\nThe way these figures are represented spatially can be visualized as follows:\r\n\r\n\r\n\r\nNotes\r\nInitial tweet shortly after I came up with the calculation idea following looking at weather data and solving integral equations:\r\n\r\nThat calculus class finally comes in handy. Figuring out a better way to measure a day’s pleasant air temperature through integration. Pretty excited about this one. pic.twitter.com/EIrXdBrHxV— Taras Kaduk (@taraskaduk) May 2, 2019\r\n\r\n\r\nR code for EDD\r\nHere is the R function to obtain the area between the temperature curve and the baseline. This function can be used in a data frame (adhering to the tidy data analysis framework) and applied with a purrr::map2_dbl() call inside a mutate() call.\r\n\r\n\r\nrequire(tibble)\r\n\r\nget_edd <- function(min, max, baseline = 18) {\r\n  \r\n  # First, create the temp. function:\r\n  a <- (max-min)/2 #amplitude\r\n  period <- 24\r\n  b <- 2 * pi / period\r\n  d <- min + a\r\n  \r\n  # This is our temperature function:\r\n  temperature <- function(x) {\r\n    -a * cos(b * x) + d\r\n  }\r\n  \r\n  \r\n  # 3 calculations based on the 3 scenarios\r\n  # of how the curve and the baseline interact\r\n  \r\n  if (min >= baseline) {\r\n    # integral <- -a*sin(24*b) + 24*d - 24*baseline\r\n    integral <- integrate(temperature, 0, 24)$value - baseline * 24 %>% \r\n      round(2)\r\n    edd <- tibble( edd_hot = round(integral/24,2),  \r\n                   edd_cold = 0, \r\n                   edd_total = round(integral/24,2))\r\n    \r\n  } else if (max <= baseline) {\r\n    integral <- baseline * 24 - integrate(temperature, 0, 24)$value %>% \r\n      round(2)\r\n    \r\n    edd <- tibble( edd_hot = 0,  \r\n                   edd_cold = round(integral/24,2), \r\n                   edd_total = round(integral/24,2))\r\n    \r\n  } else {\r\n    intercept1 <- acos((d - baseline) / a) / b\r\n    intercept2 <- (12 - intercept1) * 2 + intercept1\r\n    \r\n    integral1 <-\r\n      baseline * intercept1 - integrate(temperature, 0, intercept1)$value\r\n    \r\n    integral2 <-\r\n      integrate(temperature, intercept1, intercept2)$value - baseline * (intercept2 - intercept1) \r\n    \r\n    integral3 <-\r\n      baseline * (24 - intercept2) - integrate(temperature, intercept2, 24)$value \r\n    \r\n    edd <- tibble(edd_hot = round(integral2/24,2),  \r\n                   edd_cold = round((integral1 + integral3)/24,2), \r\n                   edd_total = round((integral1 + integral2 + integral3)/24,2))\r\n  }\r\n  return(edd)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nChow, D. H. C., and Geoff J. Levermore. 2007. “New Algorithm for Generating Hourly Temperature Values Using Daily Maximum, Minimum and Average Values from Climate Models.” Building Services Engineering Research and Technology 28 (3): 237–48. https://doi.org/10.1177/0143624407078642.\r\n\r\n\r\nClimate Impact Lab. 2018. “Estimating the Frequency of 90 Degree Days.” https://www.impactlab.org/wp-content/uploads/2018/08/CIL_Days-over-90_Method.pdf.\r\n\r\n\r\nLin, Qiaoxuan, Hualiang Lin, Tao Liu, Ziqiang Lin, Wayne R. Lawrence, Weilin Zeng, Jianpeng Xiao, et al. 2019. “The Effects of Excess Degree-Hours on Mortality in Guangzhou, China.” Environmental Research 176 (September): 108510. https://doi.org/10.1016/j.envres.2019.05.041.\r\n\r\n\r\nLivingston, Ian. 2020. “Washington Breaks Record for Most 90-Degree Days in a Month.” Washington Post, July. https://www.washingtonpost.com/weather/2020/07/27/washington-dc-july-record-heat/.\r\n\r\n\r\nMegyesi, M. S., S. P. Nawrocki, and N. H. Haskell. 2005. “Using Accumulated Degree-Days to Estimate the Postmortem Interval from Decomposed Human Remains.” Journal of Forensic Science 50 (3): 1–9. https://doi.org/10.1520/JFS2004017.\r\n\r\n\r\nPlumer, Brad, and Nadja Popovich. 2017. “95-Degree Days: How Extreme Heat Could Spread Across the World.” The New York Times, June. https://www.nytimes.com/interactive/2017/06/22/climate/95-degree-day-maps.html, https://www.nytimes.com/interactive/2017/06/22/climate/95-degree-day-maps.html.\r\n\r\n\r\nThom, H. C. S. 1952. “Seasonal Degree-Day Statistics for the United States.” Monthly Weather Review 80 (9): 143–47. https://doi.org/10.1175/1520-0493(1952)080<0143:SDSFTU>2.0.CO;2.\r\n\r\n\r\nZalom, FG, PB Goodell, LT Wilson, WW Barnett, and Bentley, WJ. 1983. “Degree Days: The Calculation and Use of Heat Units in Pest Management. University of California. Division of Agriculture and Natural Resources Leaflet.” University of California Division of Agriculture and Natural Resources Leaflet 21373.\r\n\r\n\r\nI was not aware of such computations until after writing out the logic of it. I came up with the idea independently while looking into weather patterns across U.S. cities, while simultaneously auditing a Calc III class for a refresher. My understanding is that it is best practice to cite similar research even if it was not used in researching or writing the paper.↩︎\r\nThis assumption only requires 2 data inputs (min and max daily temperature) instead of continuous temperature reading.↩︎\r\nThis assumption helps us in modeling the curve without any additional inputs, while a more precise approach would require sunrise, sunset, and solar noon times for each day (Chow and Levermore 2007)↩︎\r\nhttps://taraskaduk.com/2019/02/18/weather/ for the write-up of the methodology, and https://github.com/taraskaduk/place-to-live for the data and the R code to obtain the data↩︎\r\n",
    "preview": "posts/2019-11-11-excess-degree-days/integral.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 3324,
    "preview_height": 2670
  },
  {
    "path": "posts/2019-10-23-is-rock-dying/",
    "title": "Is Rock Dying?",
    "description": "A quantitative comparison of artists in the Billboard 200 charts 1963 - 2018. A comparison of rock and hip-hop artists show a decline of the former and the rise of the latter",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2019-10-23",
    "categories": [],
    "contents": "\r\nI’ve been a huge rock and metal fan my whole life. Most of the music that I listen to are some derivatives of rock, but mostly of a subcultural kind. I’ve been logging my listening habits for over 11 years, so in case you are curious on the validity of my claims or the details of my tastes, you can check out this post right here.\r\nAnyway, I’ve been noticing a trend in my habits for the last few years: it seemed to me that I stopped discovering new artists and pretty much listen to the same old bands. At first, I thought it’s all me to blame and I just get old. But then Finn McKenty, a guy that does reviews of subcultural music genres and various music bands on his YouTube channel “The Punk Rock MBA” (which I stan a lot), released this video titled “3 Reasons Why Rock is Dying”, and it kind of made a lot of sense to me:\r\n\r\n\r\nRegardless of the reasons Finn thinks rock is dying, it was a new thought for me - an idea that it’s not that I only listen to the old bands, but that there aren’t any new good bands to begin with. In his video, Finn cites Billboard charts as one of the data sources. In fact, he kicks it off with a phrase “Let’s look at the data”, which is totally a call to action for me. I decided to take it up and investigate it properly: not by glancing over charts, but by getting the data and analazyng it.\r\nI followed one of Finn’s hunches looked at the Billboard 200 charts and payed attention to two specific metrics: how often does rock music appear in the charts, and what is the age of the artists.1 To put the results in context, but keep it simple, I compared the data on rock with the same data on hip-hop - a music genre that we see rising more and more recently. Any other comnparisons didn’t seem especially meaningful or interesting, and so I kept it to “Rock VS Hip-Hop” exclusively.\r\nResults\r\n\r\n\r\n\r\nOne thing immediatelly jumps out: rock’s relevance in the mass market is really declining, while hip-hop is definitely on the rise. It is clear just from a sheer number of albums making it to the Billboard 200 (chart above). The picture is the same, no matter how differently we look at the data.\r\nHere is another chart, this time only looking at distinct count of artist by year (i.e. we are not counting every appearance of an album in every weekly chart, just any appearance of an artist in any given year at least once):\r\n\r\nSo, this part is clear. What about age though? Are the remaining artists that we see, are they getting older? Well, this one was not as easy to answer. The data that I have allows me to compute a real age of individual artists and a stage age of music groups.2 It would be more helpful to have start of career for both single artists and groups. But the best we can do with this data is to look at individual artists and groups separately3:\r\n\r\nWhat we see is that both rock persons and groups are aging, and so are the hip-hop groups,4 while individual hip-hop artists’ age seems to be flatlining and perhaps even reversing.\r\nConclusion\r\nSo there we have it. Indeed, there are fewer and fewer rock artists making it to the Billboard 200, and the ones that do aren’t exactly fresh blood. Of course, many subcultural genres I listen to never make it to charts like this to begin with, but the trend is visible even on the fringe.\r\nIt kind of bums me out a bit, as I wonder what will the new generations of young kids listen to. Hardcore punk, punk rock and metal music played a crucial role in my life, especially in my teenage years. It helped me channel my anger, my rebellious attitudes, helped me shape my goals, helped me grow. Pop music and hip-hop doesn’t do that for me. And I know I’ve always been on the fringe in my music tastes, and we hardcore kids are a niche of a niche and a very small minority, but I still wonder what music will be there for the new kids to rebel to? God, I hope not the same old bands I love and admire.\r\nMethodology\r\nI pulled the weekly Top 200 albums from 1963 through 2018 from https://components.one/datasets/billboard-200/. The data set is attributed to Andrew Thompson ([@asthompson](https://twitter.com/asthompson) on Twitter).\r\nHaving the weekly 200 most popular albums, I was still missing the genres of the artists and the age. I got the start date for artists via MusicBrainz API. The API also provided the genre data, but it appeared to be missing many new artists, and giving me wrong genres on some. I then resorted to the last.fm API, which has the crowd-sourced data for artists’ genres.\r\nFor each artist, I pulled the top 5 tags attributed to them, which is supposed to solve the difficulty in the genres taxonomy: most flavors of rock music will contain a “rock” tag in their top 5 tags most of the time, and same goes for other genres. (It does suggest that the same artist can be counted in more categories than one if we happen to compare data between genres. But this is not necessarily a bug, but a feature)\r\nThe R files and the data are over at https://github.com/taraskaduk/is-rock-dead (Sorry for the mess and no comments in the code, hope you can make sense of it)\r\nThe discussion of whether or not rock and metal music, especially the niche subcultures at the extremes should even be concerned with making it to the Billboard 200 is beyond the scope of this analysis. Finn provides some thoughts on this in his video, BTW↩︎\r\nThe issue was with the MusicBrainz API response on a start date of an artist: it provides a birth date for a person and a group start date for groups. I guess I could figure it out by getting every album release for every artist, and then figure out the earliest release date for each, but this is a task for too many API calls which I’m not comfotable with.↩︎\r\nI chose median age over mean age due to the fact that there is no upper bound on the data, and if an Elvis Prestley record made it to the chart today, we’d have to compute his age as of this year as 84 years old, as he’d be 84 years old when he became relevant again in Billboard 200↩︎\r\nalthough there aren’t many hip-hop groups making the Billboard 200 to begin with, which explains the high variability. Most hip-hop acts are individual performers, while most rock acts are music bands↩︎\r\n",
    "preview": "posts/2019-10-23-is-rock-dying/1.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 2483,
    "preview_height": 2391
  },
  {
    "path": "posts/2019-03-23-apple-health/",
    "title": "Analyze and visualize your iPhone's Health app data in R",
    "description": "Learn to import, analyze and visualize your Health app data in R",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2019-03-23",
    "categories": [],
    "contents": "\r\nIntroduction\r\nI am convinced that many data scientists and data analysts acquire a peculiar reflex. Any time they suspect (consciously or subconsciously) that somewhere data is being collected, they develop an irresistible itch to analyze that data, oftentimes not even knowing where the data is!\r\n\r\nAfter purchasing my first iPhone, I quickly realized that its Health app is capable of storing lots of data points. I looked at and analyzed some weight and steps data from my Android phone in the past, but this seemed like a whole new level.\r\nI Googled around and found this post from ryanpraski. The code was reproducible enough, and I ran it on my side, made a few changes and kind of left it.\r\nFast forward about 2 years, I got an Apple Watch, and got a lot more data points into my Health app. I also got a bit more serious about my workouts and my wellness, and started to rely on Health app data a lot. At one point, I even built out a small patchwork dashboard that helped me during my “bulking” and “cutting” phases: I tracked my weight, basal and active energy, calorie intake, daily amount of protein intake etc.\r\nLong story short, I accumulated enough charts and ideas to put into a blog post and share them all with the world. And so, without further ado…\r\nGet the data\r\nGetting the data out of Health app is pretty easy. From Apple Health, you can export it anywhere. I save it right away into the Files, which instantly appears in my iCloud and is accessible from my Mac.\r\n\r\nYour mileage may vary based on the path you chose to get the files. You could save it on the cloud, and then access via a web link, for instance. I find the iCloud storage synced up to your Mac being the most convenient solution. Another thing you can do is to use a Shortcut to get the Health data in a different way. For example, there is this shortcut here, but you can write your own.\r\nAnyway, back to our zip export. Now that the zip archive is on my Mac, you can break out an R session and start prodding.\r\nImport\r\nBefore I forget, a few packages you’ll need to replicate the analysis/charts are:\r\n\r\nlibrary(XML)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(scales)\r\nlibrary(ggthemes)\r\nThe path to a folder on iCloud when synced up with the cloud looks funny. You’re better off creating an RStudio project and landing your archive right in that folder. But just FYI, if you sync your Documents folder to iCloud and later want to reach the local files, the path would look like this:\r\n\r\npath <- '~/Library/Mobile\\ Documents/com~apple~CloudDocs/Documents'\r\nAnyhow, get that zip archive and unzip it\r\n\r\nzip <- paste(path, 'export.zip', sep = '/')\r\nunzip(zip, exdir = path)\r\nSys.sleep(3) # pause for 3 seconds to let your computer unzip it.\r\nNow that we unzipped the archive, we can work with the actual data. BTW, the contents of your unzipped folder will look something like this:\r\n\r\n> list.files(paste0(path,'/apple_health_export'))\r\n[1] \"clinical-records\"   \"electrocardiograms\" \"export_cda.xml\"     \"export.xml\"\r\nexport.xml is the data we’ll be using primarily\r\nI’m not entirely clear on what export_cda.xml is: it seems to contain the data similar to export.xml, but I haven’t had much luck completely parsing it. I understand CDA stands for Clinical Document Architecture and is a special format to exchange these kinds of data.\r\nelectrocardiograms is a folder where your Apple Watch ⌚️ ECGs are stored in csv format\r\nclinical-records is where external clinical results land (if you connected a clinic to your Apple Health did some lab work)\r\nBut we’ll focus solely on export.xml file\r\nFirst create an xml object containing all of the data as a starting point\r\n\r\nxml <- xmlParse(paste0(path, '/apple_health_export/export.xml'))\r\nsummary(xml)\r\nYour mileage may vary based on the amount of data you store in Apple Health, and the amount of Apple devices that use Apple Health (iPhone alone or iPhone + Apple Watch⌚). A brief rundown is the following:\r\nRecord is the main place where the data is stored. Weight, height, blood pressure, steps, nutrition data, heart rate - all stored here\r\nActivitySummary is your Apple Watch daily Activity stats: Move, Exercise, Stand data\r\nWorkout is your Apple Watch workout activity per workout logged\r\nLocation is your location logged during your Apple Watch workouts (useful for runs/hikes)\r\nInstantaneousBeatsPerMinute is exactly that: instantaneous heart rate when measured by AppleWatch\r\nExportDate is useful to validate what data are you looking at.\r\nThe rest is either some matadata or something that I haven’t found much use for (yet) You can get the data into a clean nice data frame from each attribute with a simple function here:\r\n\r\ndf_record <-   XML:::xmlAttrsToDataFrame(xml[\"//Record\"])\r\ndf_activity <- XML:::xmlAttrsToDataFrame(xml[\"//ActivitySummary\"])\r\ndf_workout <-  XML:::xmlAttrsToDataFrame(xml[\"//Workout\"])\r\ndf_clinical <- XML:::xmlAttrsToDataFrame(xml[\"//ClinicalRecord\"])\r\ndf_location <- XML:::xmlAttrsToDataFrame(xml[\"//Location\"]) %>% \r\n  mutate(latitude = as.numeric(as.character(latitude)),\r\n         longitude = as.numeric(as.character(longitude)))\r\nHere are the str() calls to these data frames:\r\nRecord\r\n\r\n'data.frame':   527744 obs. of  9 variables:\r\n $ type         : Factor w/ 34 levels \"HKQuantityTypeIdentifierActiveEnergyBurned\",..: 4 4 28 7 7 7 7 7 7 7 ...\r\n $ sourceName   : Factor w/ 8 levels \"Health\",\"MyFitnessPal\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n $ sourceVersion: Factor w/ 40 levels \"1\",\"10.2.1\",\"10.3.1\",..: 8 21 2 3 3 3 3 3 3 3 ...\r\n $ unit         : Factor w/ 14 levels \"count\",\"count/min\",..: 8 8 3 6 6 6 6 6 6 6 ...\r\n $ creationDate : Factor w/ 190384 levels \"2017-03-25 18:51:22 -0400\",..: 3313 23291 54 226 239 256 275 342 356 382 ...\r\n $ startDate    : Factor w/ 340829 levels \"2017-03-25 18:39:37 -0400\",..: 9329 31933 159 634 672 711 768 967 999 1057 ...\r\n $ endDate      : Factor w/ 341207 levels \"2017-03-25 18:46:21 -0400\",..: 9326 31926 159 634 671 711 768 967 999 1057 ...\r\n $ value        : Factor w/ 77589 levels \"0\",\"0.000102795\",..: 77165 77109 76024 65909 65912 65908 65563 65918 65918 65910 ...\r\n $ device       : Factor w/ 96622 levels \"<<HKDevice: 0x282d00000>, name:Apple Watch, manufacturer:Apple, model:Watch, hardware:Watch4,1, software:5.0.1>\",..: NA NA NA NA NA NA NA NA NA NA ...\r\nActivity\r\n\r\n'data.frame':   130 obs. of  8 variables:\r\n $ dateComponents        : Factor w/ 130 levels \"2018-11-04\",\"2018-11-05\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\n $ activeEnergyBurned    : Factor w/ 128 levels \"0\",\"1017.62\",..: 1 1 1 48 19 96 99 30 110 31 ...\r\n $ activeEnergyBurnedGoal: Factor w/ 4 levels \"0\",\"760\",\"770\",..: 1 1 1 2 2 2 2 2 2 2 ...\r\n $ activeEnergyBurnedUnit: Factor w/ 1 level \"kcal\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ appleExerciseTime     : Factor w/ 75 levels \"0\",\"1\",\"10\",\"102\",..: 1 1 1 22 73 44 40 7 38 4 ...\r\n $ appleExerciseTimeGoal : Factor w/ 1 level \"30\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ appleStandHours       : Factor w/ 12 levels \"0\",\"10\",\"11\",..: 1 1 1 3 8 7 8 7 6 7 ...\r\n $ appleStandHoursGoal   : Factor w/ 1 level \"12\": 1 1 1 1 1 1 1 1 1 1 ...\r\nWorkout\r\n\r\n'data.frame':   146 obs. of  13 variables:\r\n $ workoutActivityType  : Factor w/ 8 levels \"HKWorkoutActivityTypeHiking\",..: 8 8 2 8 8 8 8 8 7 8 ...\r\n $ duration             : Factor w/ 146 levels \"1.413170067469279\",..: 50 54 99 32 4 35 66 59 116 67 ...\r\n $ durationUnit         : Factor w/ 1 level \"min\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ totalDistance        : Factor w/ 80 levels \"0\",\"0.1136363666704027\",..: 57 63 1 47 6 14 61 66 1 7 ...\r\n $ totalDistanceUnit    : Factor w/ 1 level \"mi\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ totalEnergyBurned    : Factor w/ 146 levels \"10.9099860589808\",..: 101 141 42 74 64 65 4 8 78 131 ...\r\n $ totalEnergyBurnedUnit: Factor w/ 1 level \"kcal\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ sourceName           : Factor w/ 2 levels \"StrongLifts\",..: 2 2 2 2 2 2 2 2 2 2 ...\r\n $ sourceVersion        : Factor w/ 5 levels \"1\",\"5.0.1\",\"5.1.1\",..: 2 2 2 2 2 2 2 2 2 3 ...\r\n $ device               : Factor w/ 140 levels \"<<HKDevice: 0x282d440a0>, name:Apple Watch, manufacturer:Apple, model:Watch, hardware:Watch4,1, software:5.1.1>\"\r\n $ creationDate         : Factor w/ 146 levels \"2018-11-13 09:19:23 -0400\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\n $ startDate            : Factor w/ 146 levels \"2018-11-13 09:04:43 -0400\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\n $ endDate              : Factor w/ 146 levels \"2018-11-13 09:19:23 -0400\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\nClinical\r\n\r\ndata.frame':    17 obs. of  7 variables:\r\n $ type            : Factor w/ 3 levels \"DiagnosticReport\",..: 3 1 1 1 1 1 1 1 1 2 ...\r\n $ identifier      : Factor w/ 17 levels \"bqRS...dsJBAkJw\",..: 5 16 13 4 17 2 7 12 6 11 ...\r\n $ sourceName      : Factor w/ 1 level \"Quest Diagnostics\"\r\n $ sourceURL       : Factor w/ 17 levels \"https://api.questdiagnostics.com/resource-server/fhir/DiagnosticRepor\",..: \r\n $ fhirVersion     : Factor w/ 1 level \"1.0.2\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ receivedDate    : Factor w/ 2 levels \"2018-12-09 11:50:24 -0400\",..: 1 2 2 2 2 2 2 2 2 2 ...\r\n $ resourceFilePath: Factor w/ 17 levels \"/clinical-records/Dia...tiReport-3...2709.json\",..: 17 4 5 3 6 2 1 8 7 14 ...\r\nLocation\r\n\r\n'data.frame':   58017 obs. of  8 variables:\r\n $ date              : Factor w/ 57899 levels \"2018-11-13 09:14:50 -0400\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\n $ latitude          : num  30.3 30.3 30.3 30.3 30.3 ...\r\n $ longitude         : num  -81.7 -81.7 -81.7 -81.7 -81.7 ...\r\n $ altitude          : Factor w/ 47107 levels \"-0.0223591\",\"-0.0238451\",..: 39102 38801 38480 38045 37539 36906 36167 35364 34635 33990 ...\r\n $ horizontalAccuracy: Factor w/ 32278 levels \"0.785208\",\"0.785209\",..: 18259 17319 16544 14808 11859 10555 9507 8498 7891 7884 ...\r\n $ verticalAccuracy  : Factor w/ 32170 levels \"0.637318\",\"0.637338\",..: 23883 21283 18904 17946 16896 12473 10202 8930 7976 7775 ...\r\n $ course            : Factor w/ 29914 levels \"-1\",\"0.000919214\",..: 27796 27598 27534 29183 27827 27505 26221 27761 27833 27836 ...\r\n $ speed             : Factor w/ 47316 levels \"-1\",\"0\",\"0.000127598\",..: 21577 21574 21551 21446 21229 20885 20389 19726 18890 17951 ...\r\nWell, this is pretty much it. From here, you can let you imagination run wild. The stuff you’ll be able to do will depend on how much data you store in Apple Health and what’s the use case you have.\r\nBelow, I’ll just provide a few brief examples of what I did with the data (erasing numeric or other identifying values where necessary).\r\nExamples\r\nRecord data frame\r\n\r\n df <- df_record %>%\r\n  mutate(device = gsub(\".*(name:)|,.*\", \"\",device),\r\n         value = as.numeric(as.character(value)),\r\n         endDate = ymd_hms(endDate,tz=\"America/New_York\"),\r\n         date = date(endDate),\r\n         year = year(endDate),\r\n         month = month(endDate),\r\n         day = day(endDate),\r\n         yday = yday(endDate),\r\n         wday = wday(endDate),\r\n         hour = hour(endDate),\r\n         minute = minute(endDate),\r\n         type = str_remove(type, \"HKQuantityTypeIdentifier\")\r\n         )\r\nSee what kind of data is stored:\r\n\r\ndf %>% select(type) %>% distinct()\r\n\r\n                        type\r\n1               BloodGlucose\r\n2                     Height\r\n3                   BodyMass\r\n4                  HeartRate\r\n5      BloodPressureSystolic\r\n6     BloodPressureDiastolic\r\n7            RespiratoryRate\r\n8                  StepCount\r\n9     DistanceWalkingRunning\r\n10         BasalEnergyBurned\r\n11        ActiveEnergyBurned\r\n12            FlightsClimbed\r\n13           DietaryFatTotal\r\n14 DietaryFatPolyunsaturated\r\n15 DietaryFatMonounsaturated\r\n16       DietaryFatSaturated\r\n17        DietaryCholesterol\r\n18             DietarySodium\r\n19      DietaryCarbohydrates\r\n20              DietaryFiber\r\n21              DietarySugar\r\n22     DietaryEnergyConsumed\r\n23            DietaryProtein\r\n24           DietaryVitaminC\r\n25            DietaryCalcium\r\n26               DietaryIron\r\n27          DietaryPotassium\r\n28         AppleExerciseTime\r\n29          DistanceSwimming\r\n30       SwimmingStrokeCount\r\n31          RestingHeartRate\r\n32                    VO2Max\r\n33   WalkingHeartRateAverage\r\n34  HeartRateVariabilitySDNN\r\nWeight data\r\nFirst of all, if you use iPhone, there is no Earthly reason for you to log your weight in Google Forms or some other spreadsheet. Putting the number into Health is super easy, and I like storing things in one place (versus a bunch of orphan spreadsheets). And with the acquisition of Workflow and the renaming it into Shortcuts, it’s even easier:\r\n\r\nI’ve been tracking my daily weight since my Android times, and tried many approaches. Apple Health + Shortcuts is by far the most efficient and reliable.\r\n(And yes, you can get or write other Shortcuts to enter other data besides your weight into the Health app)\r\nAnyway, here are a few ways to visualize your data. A vanilla weight chart, for example:\r\n\r\ndf %>%\r\n  arrange(endDate) %>% \r\n  filter(type == 'BodyMass') %>% \r\n  # Had to reduce sourceName to these 2 sources to avoid double-counting\r\n  # by other apps that use BodyMass and then store it back into Health\r\n  filter(sourceName %in% c(\"Health\", \"Shortcuts\")) %>% \r\n  \r\n  ggplot(aes(x= date, y = value)) +\r\n    geom_point(alpha = 0.3) +\r\n    geom_smooth(span = 0.2, col = \"grey30\", se = FALSE) +\r\n    labs(title = \"Apple Health Weight Chart Sample\",\r\n         caption = \"@taraskaduk | taraskaduk.com\") +\r\n    theme(axis.text.y = element_blank()) # you shouldn't see these, lol\r\n\r\nWith a few additional calls to geom_vline() and geom_line() you can supercharge this chart and make it a part of a goal-tracking dashboard:\r\n\r\nMore daily grain data\r\nThere many more charts you can build from this grain: your resting heart rate, walking heart rate, VO2Max, and anything else that can be summarized to a daily level. \r\nHourly grain\r\nSome data is fun to see on an hourly grain. For example, step count and heart rate, when summarized to an hourly level, shows some cool stuff:\r\n\r\ndf %>%\r\n  filter(type %in% c('HeartRate', 'StepCount')) %>% \r\n  group_by(type, hour) %>% \r\n  summarise(value = mean(value)) %>% \r\n  ggplot(aes(x = hour, y = value, fill = value)) +\r\n  geom_col() +\r\n  scale_fill_continuous(low = 'grey70', high = \"#008FD5\") +\r\n  scale_x_continuous(\r\n    breaks = c(0, 6, 12, 18),\r\n    label = c(\"Midnight\", \"6 AM\", \"Midday\", \"6 PM\")\r\n  ) +\r\n  labs(title = \"Apple Health Data\",\r\n       subtitle = \"Hourly Data Sample\",\r\n       caption = '@taraskaduk | taraskaduk.com') +\r\n  facet_wrap(~ type)+\r\n  guides(fill=FALSE)\r\n\r\nAs these two charts nicely reveal, I have a sedentary job, I walk to work and from work, and walk around during my break, I work out after work (hence a slightly higher step count around 6 PM + higher heart rate). Or, if we further break down the step count by day of the week, it will reveal that on weekend, I move a bit all the time, versus more concentrated activity on weekdays:\r\n\r\ndf %>%\r\n  filter(type == 'StepCount') %>% \r\n  group_by(date,wday,hour) %>% \r\n  summarize(steps=sum(value)) %>% \r\n  group_by(hour,wday) %>% \r\n  summarize(steps=sum(steps)) %>% \r\n  arrange(desc(steps)) %>%\r\n\r\n  ggplot(aes(x=hour, y=wday,  fill=steps)) + \r\n    geom_tile(col = 'grey40') + \r\n    scale_fill_continuous(labels = scales::comma, low = 'grey95', high = '#008FD5') +\r\n    theme(panel.grid.major = element_blank()) +\r\n    scale_x_continuous(\r\n      breaks = c(0, 6, 12, 18),\r\n      label = c(\"Midnight\", \"6 AM\", \"Midday\", \"6 PM\")\r\n    ) +\r\n    scale_y_reverse(\r\n      breaks = c(1,2,3,4,5,6,7),\r\n      label = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\r\n    ) +\r\n    labs(title = \"Weekly Step Count Heatmap\",\r\n         caption = '@taraskaduk | taraskaduk.com') +\r\n    guides(fill=FALSE)+\r\n    coord_equal()\r\n\r\nSame can be done with a “days X months” grid, or anything else - you name it.\r\nCalorie deficit and surplus\r\nIf you’re tracking your meals in an app like MyFitnessPal, as well as having an Apple Watch, you can figure out your caloric surplus or deficit based on the data from your nutrition and your Apple Watch’s basal and active energy readings. The next chart goes a little something like this:\r\n\r\nenergy <- df %>%\r\n  filter(endDate >= '2018/11/24' & \r\n           date < '2018/12/30' & \r\n           type %in% c('BasalEnergyBurned', 'ActiveEnergyBurned', 'DietaryEnergyConsumed')) %>%\r\n  select(date, type, value) %>% \r\n  group_by(date, type) %>% \r\n  summarise(value = sum(value)) %>% \r\n  ungroup()\r\n\r\nggplot() +\r\n  geom_col(data = energy %>% \r\n             filter(type != \"DietaryEnergyConsumed\"),\r\n             aes(x= date, y = value, fill = type)) +\r\n  scale_fill_manual(values = c(\"BasalEnergyBurned\" = \"#3182bd\", \r\n                               \"ActiveEnergyBurned\" = \"#9ecae1\", \r\n                               \"DietaryEnergyConsumed\" = \"grey30\")) +\r\n  geom_col(data = energy %>% \r\n             filter(type == \"DietaryEnergyConsumed\"), \r\n             aes(x= date, y = value, fill = type),  width = 0.7, alpha = 0.6) +\r\n  labs(title = \"Calories Consumed & Burned\",\r\n       caption = '@taraskaduk | taraskaduk.com')\r\n\r\nggsave('energy1.png', width = 8, height = 6, units = \"in\")\r\n\r\nYou can also calculate given deficit/surplus by a bit of data wrangling:\r\n\r\nenergy_spread <- energy %>% \r\n  spread(type, value) %>% \r\n  mutate(EnergyBurned = ActiveEnergyBurned + BasalEnergyBurned,\r\n         EnergyConsumed = DietaryEnergyConsumed,\r\n         EnergyDeficit = EnergyBurned - EnergyConsumed,\r\n         EnergyDeficitPct = EnergyDeficit / EnergyBurned,\r\n         EnergyDeficitCat = if_else(EnergyDeficit > 0, 'Deficit', 'Surplus'))\r\n\r\nenergy_spread %>% \r\n  ggplot(aes(x= date, y = EnergyDeficitPct, fill = EnergyDeficitCat)) +\r\n  geom_col() +\r\n  scale_fill_manual(values = c(\"#1a9641\", \"#ca0020\"))+\r\n  labs(title = \"Calorie deficit and surplus\",\r\n       caption = '@taraskaduk | taraskaduk.com')\r\n(oh, hey, look, I overate a lot on Christmas!)\r\n\r\n\r\n\r\nOther dietary data\r\nSpeaking of nutrition data, you can track many various macros and micros directly from Apple Health if your nutrition tracking app is connected to it.\r\n\r\ndf %>%\r\n  filter(type %in% c('DietaryFatSaturated', 'DietaryProtein', 'DietaryCholesterol', 'DietaryPotassium')) %>% \r\n  mutate(type = str_remove(type, \"Dietary\")) %>% \r\n  group_by(type, date) %>% \r\n  summarise(value = sum(value)) %>% \r\n  \r\n  ggplot(aes(x= date, y = value)) +\r\n    geom_smooth(span = 0.7, alpha = 0.2, col = \"grey30\", se = FALSE) +\r\n    geom_point(alpha = 0.5) +\r\n    facet_wrap(~type, scales=\"free\") +\r\n    labs(title = \"Nutrition Micros and Macros to Track\",\r\n         caption = '@taraskaduk | taraskaduk.com')\r\n\r\nFor example, you could plot your protein intake against your body weight and then visualize the percentage, if you’re working out and need to ensure a specific amount of protein intake every day.\r\nOther external data: blood pressure\r\nYou can get many other things out of Apple Health data dump. For example, I use an OMRON blood pressure monitor, and it syncs the data to my Apple Health (or, I could enter the readings manually upon my health checkups if I didn’t have a monitor at home). I later can extract this BP data and visualize it:\r\n\r\nblood_pressure <- df %>%\r\n  filter(type %in% c('BloodPressureSystolic', \"BloodPressureDiastolic\")) %>% \r\n  mutate(type = str_remove(type, \"BloodPressure\")) %>% \r\n  select(value, type, date:hour) %>% \r\n  group_by_at(vars(-value)) %>% \r\n  summarise(value = mean(value)) %>% \r\n  mutate(morning = if_else(hour >=5 & hour <= 8, TRUE, FALSE),\r\n         stage = factor(if_else(type == \"Systolic\",\r\n                              case_when(value <  xxx ~ \"Normal\",\r\n                                        value <  xxx ~ \"Elevated\",\r\n                                        value <  xxx ~ \"Hypertension 1\",\r\n                                        value <  xxx ~ \"Hypertension 2\",\r\n                                        value >= xxx ~ \"Hypertensive Crisis\",\r\n                                        TRUE ~ NA_character_),\r\n                              case_when(value <  xxx ~ \"Normal\",\r\n                                        value <  xxx ~ \"Elevated\",\r\n                                        value <  xxx ~ \"Hypertension 1\",\r\n                                        value <  xxx ~ \"Hypertension 2\",\r\n                                        value >= xxx ~ \"Hypertensive Crisis\",\r\n                                        TRUE ~ NA_character_)),\r\n                      levels = c(\"Normal\",\r\n                                 \"Elevated\",\r\n                                 \"Hypertension 1\",\r\n                                 \"Hypertension 2\",\r\n                                 \"Hypertensive Crisis\")))\r\n\r\n\r\nblood_pressure  %>% \r\n  filter(morning == TRUE) %>% \r\n  ggplot(aes(x= date, y = value, group = type, col = stage)) +\r\n  geom_smooth(span = 0.7, alpha = 0.4, col = \"grey40\", se = FALSE) +\r\n  geom_point(size = 3, alpha = 0.7) + \r\n  scale_color_brewer(palette = \"RdYlGn\", direction = -1) +\r\n  labs(title = \"Blood pressure\",\r\n       subtitle = \"Systolic and Diastolic\",\r\n       caption = '@taraskaduk | taraskaduk.com')\r\n\r\nActivity data frame\r\nAnother dataset in Apple Health is your Apple Watch’s activity: Move, Exercise and Stand rings that you have daily. The grain is daily, and there is a reading and a goal for each metric (kind of redundant for Stand and Exercise, as these goals don’t change, but I guess it’s easier that way). Looking at str(df_activity):\r\n\r\n 'data.frame':  130 obs. of  8 variables:\r\n $ dateComponents        : Factor w/ 130 levels \"2018-11-04\",\"2018-11-05\",..: 1 2 3 4 5 6 7 8 9 10 ...\r\n $ activeEnergyBurned    : Factor w/ 128 levels \"0\",\"1017.62\",..: 1 1 1 48 19 96 99 30 110 31 ...\r\n $ activeEnergyBurnedGoal: Factor w/ 4 levels \"0\",\"760\",\"770\",..: 1 1 1 2 2 2 2 2 2 2 ...\r\n $ activeEnergyBurnedUnit: Factor w/ 1 level \"kcal\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ appleExerciseTime     : Factor w/ 75 levels \"0\",\"1\",\"10\",\"102\",..: 1 1 1 22 73 44 40 7 38 4 ...\r\n $ appleExerciseTimeGoal : Factor w/ 1 level \"30\": 1 1 1 1 1 1 1 1 1 1 ...\r\n $ appleStandHours       : Factor w/ 12 levels \"0\",\"10\",\"11\",..: 1 1 1 3 8 7 8 7 6 7 ...\r\n $ appleStandHoursGoal   : Factor w/ 1 level \"12\": 1 1 1 1 1 1 1 1 1 1 ...\r\nWith a bit of data wrangling, you can do lots of cool stuff. I maxed out the acceptable amount of vanilla charts in this post, so let me do another tile chart again. This will require a bit of R code though, but stay with me here.\r\n\r\n#Some data clean up  and renaming here\r\ndf_activity_tidy <- df_activity %>% \r\n  select(-activeEnergyBurnedUnit) %>% \r\n  mutate_all(as.character) %>% \r\n  mutate(date = as.Date(dateComponents)) %>% \r\n  filter(date >= \"2018-11-13\") %>% \r\n  select(-dateComponents) %>% \r\n  mutate_if(is.character, as.numeric) %>% \r\n  rename(move = activeEnergyBurned,\r\n         exercise = appleExerciseTime,\r\n         stand = appleStandHours,\r\n         move_goal = activeEnergyBurnedGoal,\r\n         exercise_goal = appleExerciseTimeGoal,\r\n         stand_goal = appleStandHoursGoal) %>% \r\n#Now, create 2 new metrics: percent of goal and a \"Yes/No\" flag.\r\n  mutate(move_pct = move/move_goal,\r\n         exercise_pct = exercise/exercise_goal,\r\n         stand_pct = stand/stand_goal,\r\n         move_bool = if_else(move_pct < 1, FALSE, TRUE),\r\n         exercise_bool = if_else(exercise_pct < 1, FALSE, TRUE),\r\n         stand_bool = if_else(stand_pct < 1, FALSE, TRUE))\r\nI’ll gather my data 3 times for each metric (and won’t write a function, go tell @drob on me), and stitch it together to get a tall dataset.\r\n\r\ndf_activity_tall_value <- df_activity_tidy %>% \r\n  select(date, Move = move, Exercise = exercise, Stand = stand) %>% \r\n  gather(category, value, -date)\r\n\r\ndf_activity_tall_pct <- df_activity_tidy %>% \r\n  select(date, Move = move_pct, Exercise = exercise_pct, Stand = stand_pct) %>% \r\n  gather(category, pct, -date)\r\n\r\ndf_activity_tall_bool <- df_activity_tidy %>% \r\n  select(date, Move = move_bool, Exercise = exercise_bool, Stand = stand_bool) %>% \r\n  gather(category, boolean, -date)\r\n  \r\ndf_activity_tall <- df_activity_tall_value %>% \r\n  left_join(df_activity_tall_pct, by = c(\"date\", \"category\")) %>% \r\n  left_join(df_activity_tall_bool, by = c(\"date\", \"category\")) %>% \r\n  mutate(category = as_factor(category, levels = c(\"Move\", \"Exercise\", \"Stand\")),\r\n         month = ymd(paste(year(date), month(date), 1, sep = \"-\")),\r\n         week = date - wday(date) + 1,\r\n         wday = wday(date),\r\n         day = day(date))\r\nNow, I can visualize this in many different ways. I like the boolean metric, and so I’ll use that:\r\n\r\ndf_activity_tall %>% \r\n  ggplot(aes(x = wday, y = week, fill = boolean)) +\r\n    geom_tile(col = \"grey30\", na.rm = FALSE) +\r\n    theme(panel.grid.major = element_blank()) +\r\n    scale_fill_manual(values = c(\"grey80\", \"#1a9641\")) +\r\n    facet_wrap(~ category) +\r\n    coord_fixed(ratio = 0.15) +\r\n    guides(fill=FALSE) +\r\n    labs(title = \"Apple Watch goals completion\",\r\n         caption = '@taraskaduk | taraskaduk.com') +\r\n    theme(axis.text.x = element_blank())\r\n\r\nOr, I could match color to activity, but then I’d need to remove all FALSE values:\r\n\r\ndf_activity_tall %>% \r\n  filter(boolean == TRUE) %>% \r\n  ggplot(aes(x = wday, y = week, fill = category)) +\r\n    geom_tile(col = \"grey50\", na.rm = FALSE) +\r\n    theme(panel.grid.major = element_blank()) +\r\n    facet_wrap(~ category) +\r\n    coord_fixed(ratio = 0.15) +\r\n    guides(fill=FALSE) +\r\n    labs(title = \"Apple Watch goals completion\",\r\n         caption = '@taraskaduk | taraskaduk.com') +\r\n    theme(axis.text.x = element_blank())\r\n\r\nYou can even do yourself a “Don’t Break The Chain” tracker, adding data from other tables and other sources:\r\n\r\nAnd since we’re on the subject, you can do something like a streak count chart, seeing how long of a streak you can get:\r\n\r\ndf_activity_streak <- df_activity_tall_bool %>% \r\n  mutate(category = as_factor(category, levels = c(\"Move\", \"Exercise\", \"Stand\"))) %>% \r\n  arrange(category, date) %>% \r\n  group_by(category, \r\n           x = cumsum(c(TRUE, diff(boolean) %in% c(-1))),\r\n           y = cumsum(c(TRUE, diff(boolean) %in% c(-1,1)))) %>% \r\n  mutate(streak = if_else(boolean == FALSE, 0L, row_number())) %>% \r\n  ungroup() \r\n\r\n\r\nggplot(df_activity_streak, aes(x = date, y = streak, group = x, col = category)) +\r\n  geom_line() +\r\n  facet_grid(category~.) +\r\n  guides(fill=FALSE) +\r\n  labs(title = \"Streak Count\",\r\n       caption = '@taraskaduk | taraskaduk.com')\r\n\r\nFinal Thoughts\r\nIf you’re a health nut and an Mac-head like me and happen have an iPhone and an Apple Watch ⌚️ - chances are there is a lot of data sitting in your Health app. Whether you want to just play with it (as much as I hate this colloquialism - it is sometimes unavoidable), or use it with some purpose - it is really easy to dig into it. Hope this post will help you get statred, as well as provide some ideas for your own purposes. Feel free to share your charts, but don’t forget to hide sensitive data (erasing a y-axis is a start)!\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-18-weather/",
    "title": "Where are the places with the best (and the worst) weather in the United States?",
    "description": "Using NOAA GSOD data in determining the amount of pleasant days in USA's core-based statistical areas",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2019-02-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nMethodology\r\nResults\r\nDiscussion\r\nUpdate 2019-12-01\r\n\r\nIntroduction\r\nIn my life, I have moved around a bit, and I always think about my next stop. There is a host of factors to consider, and I’ve seen quite a few great online tools that, for example, help you determine your ideal country based on your political views, preferences and other factors. But what about a domestic move? With a country as large and diverse as the United States, there are many places here that are quite different from one another.\r\nAbout a year ago or so, I saw a post called “The Pleasant Places to Live” (Norton 2014) showing the locations by the amount of pleasant days in a year.1 My thinking along these lines is that weather is an important factor in determining where to live. Given the weather data for cities of interest, one could define what “pleasant” weather is, and rank the locations according to such weather “pleasantness”.\r\nMethodology\r\nAfter testing quite a few different approaches, I settled on the same data source used by Norton: NOAA’s Global Summary Of The Day database (DOC/NOAA/NESDIS/NCDC > National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce n.d.). I extracted years 2012 through 2017 for all US-based weather stations, cleaned it up, filtered it out, did some averaging and fitted a linear regression for every location based on year and day of the year to fill in several missing values.\r\nI then applied this data to 929 metropolitan and micropolitan statistical areas.\r\nUsing the idea of counting pleasant days as the approach in measuring weather from sources cited above, I followed the same method. Determining ideal weather conditions is both subjective and depends on other conditions, such as setting (indoor or outdoor), activity (working, sleeping etc.) and other factors.2345678 While some studies have been performed on optimal temperature setting for some specific context (Wei et al. 2017), the overall question remains inherently subjective and arbitrary.\r\nRealizing full subjectivity of the parameters set below, my definition of a pleasant (or nice) day is that of a day when:\r\nthe max temperature was under 32°C but above 16°C\r\nthe min temperature was above 4°C but under 21°C\r\nthe mean temperature was between 13°C and 24°C\r\nno significant rain or snow\r\nResults\r\nOverall map\r\nWhile not within the scope of this analysis, I had some world data that I visualized while working on the article:\r\n\r\nSneak peak: places with most (and least) pleasant days in a year. Average of 4 years of daily data from ~29,000 @NOAA weather stations. Originally inspired by @kellegous's USA map (https://t.co/mlrmYV9NjA). Blog post coming up.#rstats #ggplot #tidyverse pic.twitter.com/8PCpRbQyvw— Taras Kaduk (@taraskaduk) June 19, 2018\r\n\r\n\r\nSpecific to the United States, the map shows that areas of most pleasant weather include the West Coast and Florida.\r\n\r\n50 best and worst\r\nMoving on to the rankings. Below is the chart design I settled on. Each chart shows 50 metropolitan and/or micropolitan areas, ranked by their average amount of pleasant days in years 2012-2017. The year 2017 is displayed for each area as a tile chart: months on y-axis, days of the month on x-axis. Areas are sorted according to the chart: from most to least in “most pleasant days”, and from least to most in “least pleasant days”: the most winner or loser is always on top\r\nTop 50 best, all metropolitan and micropolitan\r\nThis is top 50 out of all areas for which there is data. Nothing surprising at the top, with California leading the way. Tennessee was a bit of a surprise to me (these smaller towns also don’t seem to report a lot or any rain, which is suspicious). Also, it changed my frame of reference about Florida a bit: I live here now, and I consider it unpleasant (because it is very hot in the summer), but I must agree that our winters are very nice, and it looks like we’ve got it good compared to the rest of the country.\r\n[Full-size vertical version] | [Full-size horizontal version]\r\nTop 50 worst, all metropolitan and micropolitan\r\nIn the “worst” section, we see all the usual suspects: Wyoming, Alaska, Montana, North Dakota. Also, Puerto Rico and Key West, FL are the only places that are too hot: the rest is too cold. \r\nTop 50 best metro areas\r\nMicropolitan areas are not always on everyone’s mind, and therefore I wanted to look at metro areas specifically. No surprise here either, with California and Florida having the 2/3 of top 50 metro areas. The first non-California metro area is Serbing, FL, 11th in the rank. \r\nTop 50 worst metro areas\r\nOver on the other side, not so pleasant places still include Puerto Rico and the Northern USA, but now we see a lot more of Eastern Washington and Oregon, along with that cold Northeast.\r\n\r\nTop 25 best and worst metro areas with population over 1,000,000 people\r\nFinally, I ranked the biggest metro areas - the ones with the population over 1 million - in the same way. Here are 25 best and worst metro areas with over 1,000,000 people. This time, I used a different design, displaying all 6 years per metro area as “tree rings”, using polar coordinates\r\n\r\n\r\n\r\n\r\nDiscussion\r\nI feel very good about letting these imperfect charts out into the wild. Mostly, I feel liberated to be able to move on and do other things. Funny enough, one of these “other things” is to re-do this analysis, but keep it simpler and rely on existing packages rather than reinventing the wheel. Why re-do? Well, I still want to work on this project “Best place to live”, and weather is one important metric out of many. But this time around, I won’t need all the precision, all the complexity, all the ggplot wizardry: I’ll just accept a simple and somewhat imperfect metric as a proxy, as it will be one of many other numbers. So, stay tuned!\r\nUpdate 2019-12-01\r\nThis post has been referenced or credited on the following pages:\r\nDIGG.com: https://digg.com/2019/top-25-cities-most-pleasant-days-data-viz\r\nCBS13 Sacramento: https://gooddaysacramento.cbslocal.com/2019/04/30/sacramento-pleasant-weather/\r\nRevolutions Analytics: https://blog.revolutionanalytics.com/2019/03/best-and-worst-weather.html\r\nReddit:\r\nVisualization of mean daily temperatures over the past five full years for 12 Canadian cities: https://www.reddit.com/r/dataisbeautiful/comments/b6qle0/visualization_of_mean_daily_temperatures_over_the/\r\nMean daily temperatures over the past 5 years for Ottawa and 11 other Canadian cities: https://www.reddit.com/r/ottawa/comments/b6vupb/mean_daily_temperatures_over_the_past_5_years_for/\r\n\r\nMy own submission at r/dataisbeautiful: https://www.reddit.com/r/dataisbeautiful/comments/byjies/top_25_world_cities_with_most_pleasant_days_in_a/\r\n\r\n\r\n\r\nBrettschneider, Brian. 2018. “What Cities Have the Most Nice Days in America?” Washington Post, August. https://www.washingtonpost.com/news/capital-weather-gang/wp/2018/08/07/the-united-states-of-nice-days-heres-where-and-when-to-find-the-nations-most-frequent-ideal-weather/.\r\n\r\n\r\nDOC/NOAA/NESDIS/NCDC > National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce. n.d. “Global Surface Summary of the Day - GSOD.” DOC/NOAA/NESDIS/NCDC > National Climatic Data Center, NESDIS, NOAA, U.S. Department of Commerce. Accessed February 18, 2019. https://data.noaa.gov/dataset/dataset/global-surface-summary-of-the-day-gsod.\r\n\r\n\r\nNorton, Kelly. 2014. “The Pleasant Places to Live.” February 3, 2014. https://kellegous.com/j/2014/02/03/pleasant-places/.\r\n\r\n\r\nWei, Wenqi, Jackson G. Lu, Adam D. Galinsky, Han Wu, Samuel D. Gosling, Peter J. Rentfrow, Wenjie Yuan, et al. 2017. “Regional Ambient Temperature Is Associated with Human Personality.” Nature Human Behaviour 1 (12): 890–95. https://doi.org/10.1038/s41562-017-0240-0.\r\n\r\n\r\nAfter concluding my analysis, I found a similar article in Washington Post, centered around the same concept of pleasant days (Brettschneider 2018). There was no reference to the 2014 article by Norton↩︎\r\nhttps://www.reddit.com/r/askscience/comments/ulxdg/what_is_the_ideal_temperature_of_surroundings_for/↩︎\r\nhttps://www.healthyheating.com/solutions.htm#.XMnNYJO6Mne↩︎\r\nhttps://www.scientificamerican.com/article/why-people-feel-hot/↩︎\r\nhttps://www.city-data.com/forum/general-u-s/54730-what-your-ideal-outdoor-temperature-4.html↩︎\r\nhttps://health.clevelandclinic.org/what-is-the-ideal-sleeping-temperature-for-my-bedroom/↩︎\r\nhttps://www.outsideonline.com/1784591/whats-best-temperature-productivity↩︎\r\nhttps://www.sleep.org/temperature-for-sleep/↩︎\r\n",
    "preview": "posts/2019-02-18-weather/25_most_1000_polar_.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 3300
  },
  {
    "path": "posts/2018-03-29-power-query/",
    "title": "Power Query: Excel's gateway to reproducible analysis",
    "description": "Review of Power Query's scripting language as a way to evangelize reproducible data analysis and programming with minimal learning investment",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2018-03-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIntro\r\nIn this blog post, I’ll try to highlight some of Excel’s functionality which have been around for a while, but remains largely unknown to the broad public.\r\nNow, I’ll be the first one to throw rocks at the Excel camp. I’ve got receipts:\r\n\r\nPlease don't... pic.twitter.com/r3j3KQtcCT— Taras Kaduk (@taraskaduk) February 10, 2018\r\nMy analysis is in Excel. #Loseyourjobin5words— Taras Kaduk (@taraskaduk) March 5, 2018\r\nReproducible analysis, case in point. A colleague from another department is out. People come to me w/ requests to recreate some analysis that person did. I have no idea, but OK, let's see. Colleague's analysis is in Excel. How can I reproduce it? No analysis for you! pic.twitter.com/SoXZO10ekG— Taras Kaduk (@taraskaduk) November 27, 2017\r\n\r\n\r\nHowever, I’ll also be the first to jump in Excel’s defense whenever an opportunity presents itself:\r\n\r\nGenerally a very good article on advantages of R, but as always - completely overlooking Excel enhancements that came with Power Query and Power Pivot (e.g. scripting, hundreds of data connectors. I.e. all the Power BI functionality)#rstats #PowerBI https://t.co/k3qj8px81b— Taras Kaduk (@taraskaduk) December 27, 2017\r\n\r\n\r\nWhat gives? I generally agree that Excel is a bad way to conduct an analysis. Having said that, there are many enhancements to the product (Windows version mainly) that came out over the last 10 years which are completely overlooked by both Excel users and Excel bashers. One of such enhancements, Power Query (or Query Editor, or M language), allows for a reproducible data import and transformation, and is quite easy to learn. That will be this post’s topic.\r\nWhat this post isn’t about\r\nExcel cell formulas\r\nVBA\r\nPower Pivot and data modeling\r\nDAX and Excel / Power BI measures\r\nPower Query’s and M’s history\r\nfull functionality of Power Query\r\nExcel on a Mac\r\nThe audience\r\nThis post isn’t a guide to using Excel’s and Power BI’s Power Query. This post is not for people who want to learn new cool tricks of Excel.\r\nThis post is for data scientists and analysts who put reproducible analysis (achieved via programming one’s analysis in R or Python) at the center of what they do. This post is for people who like to use Excel as a punching bag (and again, I am one of those people).\r\nLastly, this post is for those analysts stuggling to switch the workflow at their place of work from Excel to R or Python. I think that Power Query could act as a gateway drug to reproducible analysis. Meet them where they’re at (in Excel), and guide them by hand out into the world of “programming your analysis”.\r\nPower Query\r\nPower Query, a.k.a. Data Explorer, a.k.a. Query Editor is Microsoft’s module within Excel and Power BI which allows users to perform data import and transformations before loading the ready tables into a workbook. It’s been around since 2013. Google for more information.\r\nComponents\r\nIn the simplest terms possible, I could break the Power Query down into two parts: the language and the GUI.\r\nThe M language\r\nFrom MSDN:\r\n\r\nThe Power Query M formula language is optimized for building highly flexible data mashup queries. It’s a functional, case sensitive language similar to F#\r\n\r\nIn simpler terms, it is a data transformation language. Now, I’m not a computer scientist, and won’t be able to explain all the technical details well, therefore I suggest that the most interested ones go and check out the Power Query M language specification. For the rest of us, I’ll just say that M works by calling a function on a table or a list, then storing this result as a new table, and then calling this new table in the next step with another function. That is oversimplification, of course, but for the purpose of this post, it should do.\r\nExample 1\r\nLet me explain it on an example. In Excel, I created sample table of 3 rows and 3 columns called df. \r\nI then loaded it into the query editor, and pressed a few buttons. Here is the code it generated (I edited the step names and indented the lines):\r\n\r\nlet\r\n    Source = Excel.CurrentWorkbook(){[Name=\"df\"]}[Content],\r\n    change_type = Table.TransformColumnTypes(Source,\r\n                                            {\r\n                                              {\"a\", Int64.Type}, \r\n                                              {\"b\", Int64.Type}, \r\n                                              {\"c\", Int64.Type}\r\n                                            }),\r\n    filter = Table.SelectRows(change_type, each ([a] <> 7)),\r\n    remove_cols = Table.RemoveColumns(filter,{\"c\"})\r\nin\r\n    remove_cols\r\nThe final output will look like this: \r\nThe first step, Source, is our import step. It tells Power Query where to find our table. The second step, change_type\", is auto-generated. Notice that it references the first step as the first argument of the function Table.TransformColumnTypes: it says “that’s the table we will work with”. All this step does is assigns columns a, b and c the type of integer. Next step, filter, references the previous step, and performs a filter operation. Finally, remove_cols takes the result of the previous step, and then removes a column. Then, the code tells Power Query that the result of remove_cols is the one to be printed.\r\nThis is a very basic explanation of how this code works. You can twist it, bend it to your will, skip steps, branch out, use parameters etc. But the common functionally is this stitched freight train-like sequence of steps.\r\nExample 2\r\nHere is the data transformation sample from David Robinson’s DataCamp course on the tidyverse\r\n\r\n\r\n\r\nI hope I don’t need to read to you what it does (if you can’t read the code, try running it. If you have no idea what’s going on here - I suggest taking the above-mentioned David’s class on the tidyverse)\r\nHere is how I’d solve the same simple task in Power Query. First, Power Query in Excel, unlike Power Query in Power BI, can’t run R scripts, therefore I can’t just load a package. But Power Query can read .RData files. It can also load stuff from the web. We’ll do just that\r\n\r\nlet\r\n    Source = Web.Page(\r\n                Web.Contents(\"https://github.com/jennybc/gapminder/blob/master/inst/extdata/gapminder.tsv\")),\r\n    Data = Source{0}[Data],\r\n    col_types = Table.TransformColumnTypes(Data,{\r\n                                                  {\"\", type text}, \r\n                                                  {\"country\", type text}, \r\n                                                  {\"continent\", type text}, \r\n                                                  {\"year\", Int64.Type}, \r\n                                                  {\"lifeExp\", type number}, \r\n                                                  {\"pop\", Int64.Type}, \r\n                                                  {\"gdpPercap\", type number}\r\n                                                }),\r\n    filter = Table.SelectRows(col_types, each ([year] = 2007)),\r\n    mutate = Table.AddColumn(filter, \"lifeExpMonths\", each [lifeExp] * 12, type number),\r\n    arrange = Table.Sort(mutate,{{\"lifeExpMonths\", Order.Descending}})\r\nin\r\n    arrange\r\nNow, again, I changed the names of the steps and indented the code for readability purposes. The rest was generated by Power Query and I was just clicking on things. I want to stress it out again: I didn’t have to know any of the functions, any of the syntax. All I did was:\r\nPass a web link into GUI.\r\nFrom there, Power Query figured out that it needed a combo of Web.Page(Web.Contents()) to get to the data. It saw a table and guessed column types for me.\r\nFrom here, I clicked on the “year” column header to filter it, clicked a button to create a new column out of the old one, and the clicked on its header to sort in descending order.\r\nIn other words, I came to this with no pre-existing knowledge of coding, and got myself a reproducible piece of code. The data refreshes upon each load: Excel will be checking Jenny’s GitHub page every time we refresh the data, and will be applying the steps as documented.\r\nThe GUI\r\nAs you may have guessed from my previous paragraph, the Query Editor GUI is bread and butter of this whole scheme: the M language itself is hard to type by hand, the functions are long, it is case sensitive, and there is no good source code editor (Notepad++ and other text editors do a better job than the Power Query itself). But I feel like Power Query wasn’t built to program in: that’s not the main customer base of Excel and Power BI. What Power Query is good at is its GUI that allows users to click around and apply data transformation steps, all the while generating a script behind the scenes.\r\nFirst, there is a ribbon with several tabs and plenty of buttons to click on. Some represent very simple existing functions, while others are pretty complicated and generate a solid chunk of code on just one click.\r\n\r\nNext, you are allowed to interact with your data to some extend. You can’t edit any cells, but you can filter columns, move them around, fill them down, sort, and so on, within the table itself. Power Query will pick up on your actions and will save your transformations in a script.\r\n\r\nYou can have more than one query, coming from different sources, and you can make them interact with each other: merge (join), append (union), reference, split, nest and so on.\r\nYou can also re-arrange the query steps in the GUI via simple drag-n-drop, and your script will be re-written to reflect the new order.\r\nWhat I like about Power Query\r\nThere are quite a few things I like about Power Query:\r\nLearning curve. It’s easy to get started with Power Query and create functional reproducible scripts out of the gate. The powerful GUI allows for that. No setup necessary: no installation, fine tuning, no ODBC drivers and connections. It just works out of the box. Almost like it wasn’t Microsoft creating it.\r\nAccessing data. Getting the data is the most seamless experience I’ve had. It has pre-built functions for many data sources (I’ve got receipts: here is the list of functions), and can recognize a large amount of data formats. It doesn’t require any ODBC setup. The Power Query flavor that runs on Power BI can also use R scripts as a data source or a data step, yet this functionality isn’t a part of Excel yet.\r\nTidy-like data storage and display. Surprised? Yeah, with a few exceptions, Power Query treats everything as a data frame. What if it’s not a data frame? Then it tries to fit the data into a rectangular shape. I’d like to show a few examples.\r\nHere is an example of how Power Query treats JSON files. I used the well-known (thanks to Jenny Bryan’s tutorials!) API of Ice and Fire. Here is what I’ve got after a few clicks. Note that I only plugged in the API call as a URL - Power Query did the rest.\r\n It is a data frame, but it has a nested list column for titles. Neat!\r\nNot only JSON records get nested. You could have a nested table (can happen upon a join or after a group_by-like call), or a nested list. Regardless, Power Query will always try to make your data rectangular, which is pleasing to any tidyverse adept. Here is another example. I took the same Gapminder dataset, and nested it, grouping by country. The table above is now how Power Query sees the table. The data frame below is a sneak peek into one of the nested cells for the United Kingdom. The function up on top is the step I applied to nest the data frame:\r\n\r\nOne more example. Here is how Power Query sees a folder full of files:\r\n\r\nLikewise, if you told Power Query to access a database, and didn’t specify SQL statement, it would return a data frame of all tables and views in that database, and you can take it from there.\r\nRobust GUI that keeps the code. Another cool feature is that the GUI can handle 90% of one’s needs, and it scripts all transformations behind the scenes. I think of it as a gateway drug to “programming one’s analysis”\r\nWhat I don’t like about Power Query\r\nSpeed. It is unbelievably slow. I guess the convenience comes at a cost of performance. It does well on small datasets and simpler operations, but fails to do a decent job the moment you scale.\r\nSyntax and flexibility. The language is rather rigid, and the syntax is annoying at times.\r\nLimits. It can only do so much. You can import and transform the data, but you can’t do anything else here.\r\nConclusion\r\nWhat we have these days is several generations of knowledge workers trained on doing their analysis in Excel. Just bashing the tool is not productive. We think we provide better alternatives with R or Python, but we frequently forget about the learning curve associated not only with learning a new language, but also with learning a language for the first time. Excel’s Power Query could serve as an important stepping stone in taking the analysis out of the Wild West world of Excel VLOOKUPs into the world of reproducible code, git repos and other warm and fuzzy things.\r\n\r\n\r\n",
    "preview": "posts/2018-03-29-power-query/cover.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 1461,
    "preview_height": 789
  },
  {
    "path": "posts/2017-11-26-pixel-maps/",
    "title": "Create World Pixel Maps in R",
    "description": "A walk-through of generating personalized pixel maps with R from scratch.",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2017-11-26",
    "categories": [],
    "contents": "\r\nToday, I’m going to show you how to make pixel maps in R. Why pixel maps? Because they look awesome!\r\nI was searching on the web for a while, but couldn’t find a good tutorial. Being stubborn as I am, I eventually figured out a way to get what I want. You know, if you torture your code enough, it might give you what you need.\r\nSetup\r\nThe workflow here is very lightweight, and only requires tidyverse and maps to be loaded.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(maps)\r\n\r\n\r\n\r\nPersonal coordinates\r\nNext, as we are trying to make this map personal, you’ll need to supply a list of data points to map: places you’ve lived, places you’ve traveled to, and so on. I will leave you to your own devices here, whether you want to do some reverse geocoding, or look coordinates up one by one is up to you. The desired outcome here is a data frame containing latitude and longitude coordinates of the places you want to map.\r\n\r\n\r\n\r\n\r\nRounding the coordinates\r\nAs I’m creating a pixel map - I need dots in the right places. I’m going to plot a dot for each degree, and therefore I need my coordinates rounded to the nearest degree.\r\n\r\nFor smaller maps, consider a dot for each half of degree, in which case, round up to a .5 degree with a formula of y=round(x/5,0)*5\r\n\r\n\r\n\r\nGenerate a pixel grid\r\nThe next step is the key to getting a pixel map. We are going to fill the entire plot with a grid of dots - 180 dots from south to north, and 360 dots from east to west, but then only keep the dots that are on land. Simple!\r\n\r\nIf you consider a dot for each half a degree, then change the step from 1 to 0.5\r\n\r\n\r\nlat <- tibble(lat = seq(-90, 90, by = 1))\r\nlong <- tibble(long = seq(-180, 180, by = 1))\r\ndots <- lat %>% \r\n        merge(long, all = TRUE)\r\n\r\ndots <- dots %>% \r\n        mutate(country = map.where('world', long, lat),\r\n               lakes = map.where('lakes', long, lat)) %>% \r\n        filter(!is.na(country) & is.na(lakes)) %>% \r\n        select(-lakes)\r\n\r\n\r\n\r\nAt this point, you are pretty much done with prep work. The next step - creating the visuals - will be highly dependent on your data, your needs, and your wants.\r\nPlotting\r\nAs said earlier, this is where we part ways and you are left on your own. I will provide my personal workflow as an example, but it may not work best for your needs. It’s up to you and your creativity now!\r\nTheme\r\nI like defining a theme upfront. Mainly to remove all unnecessary chart features and define main colors.\r\n\r\n\r\ncolor_bk <- \"#212121\"\r\ntheme <- theme_void() +\r\n        theme(panel.background = element_rect(fill=color_bk),\r\n              plot.margin = unit(c(0, 0, 0, 0), \"cm\"))\r\n\r\n\r\n\r\n\r\n\r\nplot <- ggplot() +   \r\n        #base layer of map dots\r\n        geom_point(data = dots, \r\n                   aes(x=long, y = lat), \r\n                   col = \"grey45\", \r\n                   size = 0.7) + \r\n        #plot all the places I've been to\r\n        geom_point(data = locations, \r\n                   aes(x=long_round, y=lat_round), \r\n                   color=\"grey80\", \r\n                   size=0.8) + \r\n        #plot all the places I lived in, using red\r\n        geom_point(data = locations %>% \r\n                     filter(flag == 'lived'), \r\n                   aes(x=long_round, y=lat_round), \r\n                   color=\"red\", \r\n                   size=0.8) +\r\n        #an extra layer of halo around the places I lived in\r\n        geom_point(data = locations %>% \r\n                     filter(flag == 'lived'), \r\n                   aes(x=long_round, y=lat_round), \r\n                   color=\"red\", \r\n                   size=6, \r\n                   alpha = 0.4) +\r\n        #adding my theme\r\n        theme\r\n\r\n\r\n\r\n\r\n\r\n\r\nA large map like this is not particularly appealing. Zooming in or, say, removing Antarctica, could be a good approach.\r\n\r\n\r\nplot + scale_y_continuous(limits = c(10, 70), expand = c(0,0)) +\r\n        scale_x_continuous(limits = c(-150,90), expand = c(0,0))\r\n\r\n\r\n\r\n\r\nFinal comments\r\nObviously, there is so much more to do with this. The possibilities are endless. The basic idea is pretty simple - generate a point grid and plot rounded coordinates on top of the grid.\r\nLet me know if you find new implementations of this code!\r\nUpdate, 2020-09-07\r\nSince its publication, this blog post has been referenced on the following pages:\r\nPixel Maps in R: https://paulvanderlaken.com/2018/01/31/pixel-maps-in-r/\r\nPixel/Symbol Map Magic with ggplot: https://dadascience.design/post/r-pixel-symbol-map-magic-with-ggplot/\r\nhttps://github.com/lhehnke/tidytuesday\r\n\r\n\r\n\r\n",
    "preview": "posts/2017-11-26-pixel-maps/pixel-maps_files/figure-html5/plot-1.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 3840,
    "preview_height": 1920
  },
  {
    "path": "posts/2017-09-30-mpaa/",
    "title": "Do MPAA movie ratings mean anything?",
    "description": "Investigating the movie ratings and and their content. With R.",
    "author": [
      {
        "name": "Taras Kaduk",
        "url": "https://taraskaduk.com"
      }
    ],
    "date": "2017-09-30",
    "categories": [],
    "contents": "\r\nBeing a parent in modern days is lots of fun. Not only all of us are pretty much winging it, not having any idea what we’re doing1  — we are also constantly being watched and judged by other parents.\r\nWhen it comes to watching movies with our six-year-old son, we don’t have a strict set of rules. We pretty much fly by the seat of our pants with “I know it when I see it” approach to violence, profanity, or any other content. Not to say that we’re watching Pulp Fiction and Basic Instinct (the most challenging movie to date was probably Alice in the Wonderland), but all the movies we watch with our son are between G and PG - and we hardly can tell a difference between the two.\r\nThat’s why I was surprised to find out that some parents swear by this MPAA rating system, and use it religiously when deciding what their kids can and can’t watch.\r\nAnd it’d be all good if I haven’t noticed that these ratings are sometimes kind of… arbitrary? So, I decided to dig into the data. Because data will solve all of our problems, right?\r\nData set\r\nI searched around a bit, and stumbled upon this awesome website called kids-in-mind.com. It had a lot of info similar or equal to one contained on IMBd or Common Sense Media, but it had a crucial key component: every movie on this website is rated on an 11-point scale, from 0 to 10, on three metrics: sex & nudity, violence & gore, and profanity. Well, this is just perfect! Not only that — it also has that MPAA rating data point for every movie, which means I get all of my data in one sitting.\r\n\r\nSince the writing of this post, kids-in-mind.com has been drastically redesigned to the point where my scrapping code no longer works. And even if it worked, I still choose not to share the scraping script. Please inquire the data from kids-in-mind.com personally.\r\nSo, I wrote a little R script using rvest package, and got my data into a tidy data frame, and started exploring. After a little bit of data wrangling (I excluded NR movies as they are obviously not rated, and are all over the place. Also, Kids In Mind database didn’t have many NC-17 rated movies, therefore I combined them with R rated films), I got my first results.\r\nResults\r\nOn average, higher MPAA rating follows higher levels of inappropriate content, but…\r\nThe first result seemed fairly obvious: higher (stricter) MPAA ratings have a higher rate of violence, sex and profanity. On average. However, the amount of overlap is astonishing. Basically, any category is entirely consumed by its two neighboring categories.\r\nWhat’s more, you can always find a movie in a “lower” category that is more inappropriate than some other movie in a “higher” category: Jimmy Neutron VS Little Rascals, the 5th Harry Potter VS Life is Beautiful, Year One VS The King’s Speech etc.\r\nYou can see this from the figure below. You may also notice that there movies scoring 2.5 points on average that are in every MPAA category. We’ll come back to this later.\r\n\r\n\r\n\r\nMPAA is most forgiving on violence\r\nWell, no kidding! This was hardly a surprise. As a foreigner, I am constantly amused by how much violence is considered appropriate, contrasted with, for example, how little nudity is acceptable. Guts and blood? Body parts? Sure, bring it on! Naked breasts? How dare you!\r\nSo, next time you rent a G rated movie and think it is clean - think again. It’s probably just as violent as that other PG movie you wanted. Both G and PG movies center around 3 points on violence anyway, with max points being 5 for G and 6 for PG. Just go with PG then, eh?\r\n\r\n\r\n\r\nWhat the **** is up with profanity?\r\nNow, this is a zero tolerance zone in the movie world. Not sex and nudity, as I assumed. Profanity. Unlike other categories, where scores flow gradually from category to category, profanity has some clear trends:\r\nAll G movies are bundled up in a narrow 0-2 points corridor\r\nMost PG-13 movies are between 4 and 5 points on profanity\r\nR and NC-17 movies reside between 5 and 10 points\r\nI bet if I was trying to predict an MPAA rating based on these criteria, profanity would be the strongest predictor (not a concern of this post, but maybe later)\r\n\r\n\r\n\r\nLooking at R & NC-17 section, it is tempting to dive in a bit more. Let’s go!\r\n\r\n\r\n\r\nIndeed, movies in R & NC-17 categories are widely distributed across violence and sex, but snuggle tightly in the upper section of profanity. Why is that? Looking at the data, we can tell that often profanity accompanies other “R” worthy content. However, it is not always the case, and correlation is relatively weak. Good Will Hunting is neither violent nor sexually explicit, but it is profane AF, and, sure enough, is R rated for - wait for it - “strong language, including some sex-related dialogue”. It could be just me (after all, I am a foreigner, and English words don’t carry the same connotation for me), but I think it is mighty unfair to Good Will Hunting to be rated R, especially knowing that Scary Movie, parts 3 through 5, are rated PG-13.\r\nSummary\r\nSo, what have we learned?\r\nIt is probably OK to use MPAA ratings as a guide\r\nIf you’re optimizing for lack of violence, G and PG movies aren’t that much different, therefore don’t worry much.\r\nR rating doesn’t mean the movie is violent or has a lot of sexual content. But it definitely means there is some profanity in it!\r\nCaveats\r\nIt is important to remember that any rating will be arbitrary a priori. We aren’t working with exact count of swear words, time duration of violent scenes, or percentage of naked body revealed. And kids-in-mind.com rating isn’t perfect either. For example, the website rates Pulp Fiction at 10 on a “sex & nudity” scale, while there is hardly any sexual content in the movie.\r\nUpdate, Jan 10, 2018\r\nSo, a few days ago we were watching The Late Show with Stephen Colbert, and this bit with Matt Damon caught my instant attention:\r\n\r\nI remember when people at MIRAMAX came to us and said “Could you make it [Good Will Hunting] PG-13?” There’s no violence or sex to speak of, it’s just… And I said “What’s making it rated R?”, and they said “the language”, and I said “Okay well so we could loop a couple lines”, and they go “Yeah but you’re only allowed” … I think at the time you were allowed to say the F-word three times… and I said “Okay, well how many are we off by?” And they said “You go over by a hundred and forty-five”\r\n\r\n\r\nHa! So, my theory checks out! It’s profanity that makes a movie R rated! It can be puritan and pacifistic, but you drop a couple of F-bombs — and you’re out.\r\nIt is funny that I chose exactly Good Will Hunting as an example of how an otherwise modest movie can be sent straight to the R bench for what Matt claims is how they all talk in Boston.\r\nSeriously, you need a license to do braids and nails, yet raising a human being a future member of society is a no-brainer, right?↩︎\r\n",
    "preview": "posts/2017-09-30-mpaa/mpaa_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-06-06T14:01:24+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
